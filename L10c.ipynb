{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339L, 784L)\n",
      "(1850L, 784L)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "       # print \"Debug ANDRZEJ:\", input\n",
    "        #wynik = np.asmatrix(np.vectorize(lambda x: x if x >= 0 else 0)(input))\n",
    "        #wynik = [x if x>=0 else 0 for x in input]\n",
    "        input[input<=0]=0\n",
    "        #print \"Debug Andrzej:\", input\n",
    "        return input\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input[input<=0]=0\n",
    "        input[input>0]=1\n",
    "        print \"tester:, \", output_grad.shape\n",
    "        return  np.asmatrix([ np.multiply( output_grad[i,:],   input[i,:]   )\n",
    "                          for i in xrange(input.shape[0])]), None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def sigmoid(gamma):\n",
    "    return expit(gamma)\n",
    "    #if gamma < 0:\n",
    "    #    return 1. - 1./(1. + math.exp(gamma))\n",
    "    #else:\n",
    "    #    return 1./(1. + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        f = np.vectorize(lambda x: sigmoid(x) , otypes=[np.float])\n",
    "        return f(input)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        \n",
    "        f = np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) , otypes=[np.float])\n",
    "        #print \"Testerhehe\", output_grad.shape\n",
    "        input_grad = np.asmatrix([ np.multiply( output_grad[i],  \n",
    "                    f(input[i,:])   )  for i in xrange(input.shape[0])])\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            #np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            #np.random.seed(1)\n",
    "            self.weights = np.random.choice([0.001, 1000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(np.array([[1] for i in input]), input, axis=1)\n",
    "        return np.dot(input, self.weights)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        input = np.append(np.array([[1] for i in input]),input, axis=1)\n",
    "        n = input.shape[1] \n",
    "        m = output_grad.shape[1]\n",
    "         \n",
    "        print \"output_grad.shape: \", output_grad.shape\n",
    "        input_grad = np.asmatrix(np.dot(output_grad, (self.weights[1:,:]).T))\n",
    "        print \"input_grad.shape: \", input_grad.shape\n",
    "        #weight_grad=  np.asmatrix( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n",
    "                               #    for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "       \n",
    "        weight_grad = np.dot(input.T,output_grad)\n",
    "        \n",
    "        return input_grad, weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(np.linalg.norm(delta_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = 0\n",
    "        self.ready = False\n",
    "    def calculate_deltas(self, grad):\n",
    "        #if self.ready == False:\n",
    "        #    self.remember = grad\n",
    "        #    self.ready = True\n",
    "        \n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(-1.0*np.multiply(t, np.log(y))\n",
    "                          -np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        return ( -1.0*np.divide(t,y) + np.divide((1.0-t),(1.0-y)) )/ (1.0 * y.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False,plot=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        times = 0\n",
    "        norms = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                times+=1\n",
    "                #norms = norms.append()\n",
    "        \n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        if plot:\n",
    "            nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "            maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "            plt.figure(figsize=(20,tmp), dpi=80)\n",
    "            plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            iteri = 1\n",
    "            for i, layer in enumerate( self.layers):\n",
    "                if not layer.__class__.__name__ == \"Dense\":\n",
    "                    continue\n",
    "                if layer.plot:\n",
    "                    plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                    iteri+=1\n",
    "                    plt.title(\"Layer \"  +str(i))\n",
    "                    plt.ylim([0, maximaxi])\n",
    "                    plt.yscale('symlog')\n",
    "                    plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "\n",
    "        grad = self._join(layer_grads)\n",
    "        \n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, deltas):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "        \n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "           # print \"Attention: \", inps[-1].shape\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "        print \"kosmo test:, \", inps[1].shape\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (232L, 8L)\n",
      "Debug:  (232L, 1L)\n",
      "Debug:  (232L, 8L)\n",
      "    -> loss: 0.098259\n",
      "    -> metric: 0.862000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (232L, 8L)\n",
      "Debug:  (232L, 1L)\n",
      "Debug:  (232L, 8L)\n",
      "    -> loss: 0.079587\n",
      "    -> metric: 0.913000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (256L, 8L)\n",
      "Debug:  (256L, 1L)\n",
      "Debug:  (256L, 8L)\n",
      "kosmo test:,  (232L, 8L)\n",
      "Debug:  (232L, 1L)\n",
      "Debug:  (232L, 8L)\n",
      "    -> loss: 0.064261\n",
      "    -> metric: 0.941000\n",
      "0.935135135135\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "def metric2(y,t):\n",
    "    return mean_squared_error(np.around(y.flat), t)\n",
    "\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train[:1000],y_train[:1000], epochs= 3, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-bd05d3d82282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-dc9c7c90dbc1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats, plot)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> loss: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> %s: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnDenses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Dense\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-91ae78931834>\u001b[0m in \u001b[0;36mmetric2\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmetric2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\regression.pyc\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 231\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    232\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[1;32m    233\u001b[0m                                weights=sample_weight)\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\regression.pyc\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric2])\n",
    "network1.add(Dense(784,16))\n",
    "network1.add(Dense(16,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.000000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '_collapse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-34d4749de6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnetwork2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-dc9c7c90dbc1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats, plot)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mbatch_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mtimes\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[1;31m#norms = norms.append()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-dc9c7c90dbc1>\u001b[0m in \u001b[0;36m_fit_on_batch\u001b[0;34m(self, batch_X, batch_t)\u001b[0m\n\u001b[1;32m     75\u001b[0m         layer_grads = self._backward_pass(\n\u001b[1;32m     76\u001b[0m             \u001b[0minps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-dc9c7c90dbc1>\u001b[0m in \u001b[0;36m_backward_pass\u001b[0;34m(self, inps, grad)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0minput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mweight_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mweights_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0minput_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-80fe098d5deb>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, input, output_grad)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0minput_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         weight_grad=  np.asmatrix( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n\u001b[0;32m---> 36\u001b[0;31m                                    for b in xrange(m)]  for a in xrange(n) ] )  \n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1843\u001b[0m             return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[1;32m   1844\u001b[0m                                 out=out, **kwargs)\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         return _methods._sum(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.pyc\u001b[0m in \u001b[0;36msum\u001b[0;34m(self, axis, dtype, out)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collapse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_collapse'"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network2.add(Dense(784,32))\n",
    "network2.add(Dense(32,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Nie ma sensu ReLU jako ostatnia warstwa, ponieważ rozważany problem to problem klasyfikacji binarnej. Zwracane wartości to powinno być 0 lub 1, ew \"prawdopodobieństwo\". ReLU wyrzuca cokolwiek dodatniego. Być może dużego. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.697375\n",
      "    -> metric: 0.500000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.697655\n",
      "    -> metric: 0.500000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.697940\n",
      "    -> metric: 0.500000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBsAAAFyCAYAAAC0gdLxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAHNZJREFUeJzt3XuUZWV5J+Df29yVi4Lc5GJHQQXNgqhEQSWBjEZFGRJ1\nnBiDEq+MrsSgs5CsFXWCo6ArhjGOCLEjMBNXUJkgAuNlEEZm0AzogOANUUsugkbkOiKkwzd/nNOd\n6uqqrqrur6p2dT/PWmdR5+x9vnr3x6l3H37sS7XWAgAAANDLiqUuAAAAANi8CBsAAACAroQNAAAA\nQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjbAAqqqK6rqPUtdx2yqav+quriq\n7quqn1XVh6tq26WuC9gyLKNe+bGq+mZVra6q/7rU9QBbluXQK6tqx6q6vKp+UlX3VtUtVfWXVbX9\nUtfG4hM2wBaiRtYLEKpqRZKLk/w8yT5Jnp7kyCQfWNwKAZbeTL1y7BtJTkpy0SKWBDA4G+iVDyb5\noyT7tdZ2TnJYkqcl+Y+LWR/DIGyAJVJVT62qy6rqH6vqnqr6h6o6etLyK6vqXVPe87JxUrzt+Pkz\nxyn3nVX1o6o6taq2nrR+q6o/qaqrktyf5HenKeW5SQ5KclJr7d7W2o+S/FmS10mhgaU2oF6Z1tqH\nWmufT3LvgmwswEYaSq9srf1Ta+361tpDk15+OMmTem8zwydsgKV1WpL9k+yR5L8n+fuq2mO87Mwk\nrx0febDGG5N8vLX2UFU9KcllST6aZM+MjkY4NsnJU37HG5O8LsmOST4zTQ2HJvlBa+1nk167Oskj\nkjxxE7YNoJch9EqAoRtMr6yqv62q/5fk9iSHJHn/Jm4by5CwAZZIa+2G1toXW2sPtNYebK29O0lL\n8szxKp9Osn2SFyZJVT0hyVFJzh4vf3OSz7bW/q61tnp8RML7k5ww5Vf9ZWvtW23kgWlK2TnJ3VNe\nu2vSMoAlM6BeCTBYQ+uVrbXfzyiQODTJWUlu7rKhLCtbz74KsBCqav+MmvgRSR6V0SFmO2eURmec\nMq/KKEG+JMkbklzWWvvBeIgDkxxVVZODghVZP0T84Syl3Dv+/ZM9etIygCUzoF4JMFhD7JWttZbk\nuvFRExdkdF0wtiDCBlg6f53kniSHtdZ+UlWV0REFNWmds5J8t6oen1Gy/MZJy+5I8onW2h/O8nse\nnmX5tUl+pap2a63dOX7tsCS/SHLj3DYFYMEMpVcCDNmQe+U2cc2GLZLTKGDhbVVV2095rEiyS0YX\n17mrqh6Z5H0ZHW62VmttIskXk3wqyT8l+eykxR9J8rKqenlVbVtVW1XVAVX1gnnWd2WS7yT5i6ra\nqaoel+TUJKtaa7/ciO0F2BhD75UZv3/7JFslWTGucbuN2ViAjTToXllVv15Vz6uqR1TViqp6epJ3\nJbl0YzeY5UvYAAvvHUkemPI4OqPbAh2SUer8rSS3Jbl1mvefmdEtg1a11lavebG1dnWS5yV5/fi9\nd2Z0Pt7j5lNca+3hJC9JsntGF/H5epIvJ/n38xkHYBMNuleOfWFc16uS/N745+9uxDgAG2vovXLb\nJO/N6DvlPUnOz+hCkq+d5zhsBmp0Kg0wVFV1UJIbkvxKa83FdQCmoVcCzE6vZDEJG2DAxvc9XpVk\n29baK5a6HoAh0isBZqdXsticRrEIquo5VfXVqrqqqt621PWwPFTVMRkdCveUJD43bPb0SjaGXsmW\nRq9kY+iVLAVHNiyCqnpskp+NbzlzeZJjWmu/WOq6AIZErwSYnV4JLBdufbkIWms/nvT0n+P2WgDr\n0SsBZqdXAsvFnE+jqKovVNU3quraqrqyqn5thvVeUFXXjNf9alUdMn59t/F71zxurKrVVbXrXMev\nqhOqqlXVcXMZc2NV1YeqamL8uw6dsuzA8WFrN1bV1VX1lHmM+7wk33c7Qdh86ZVrl+mVwIz0yrXL\n9EpgszXn0yiq6lGttbvHP/9Okne31g6Zss6jk9yU5MjW2jer6rlJzmytPXWa8d6e5Ddaay+Zy/hV\ntTLJJ5JUktNbaxfONuaUZdsm2ae19sNJr22XZO/xPWcnr3tkkh8k+V9JjmutXTtp2ZeSnNdaO6eq\nXpbk5NbaYVV1cEb3p53sc62108bv2zfJeUmOba3dP019JyU5KckOSXZYsWLFDnvvvffU1Tbojnvs\nazZkr1223+QxzPGG9Zjj5e7hhx/OihWjHPeBBx7Ivffemz333HO9de64447svvvu2WabbfLggw/m\n7rvvXm+9JLnvvvvy0EMPZbfddpvT+KtXr85dd92V1lp22mmn7LDDDrntttseaq1tt2adLb1XAktP\nr1y7TK8ElpWpvXJDNuqaDVX1miRvba1NTWefkeQTrbUnTnrt3iS/2Vr7+pR1v53klBlCg3XGr6oV\nGd3b+uQkf5HkjBnet6ExD01yQUZN/vqqekSSC5Nc1lo7fYbtnMiknUJV7ZFRmLJra211VVVG95B9\nTmvtpunGGL9vuySXJHlza21O9+Ped9992623Tndr3JmtfMcl81p/SzNx2jGbPIY53rAec7w5Oeec\nc3LGGWfk2muvXef1a665Jq985Stz4403rn1t5513zhVXXJGnPe1p66x70EEH5X3ve1+OO+64Wcd/\n+OGH8/znPz+nn3563va2t+Wtb31rjjvuuFTVba21fde8b0vvlcCw6JV6JbB8TO2VGzKvazZU1XlJ\njho/fdE0q3wvyW5VdURr7aqqOjbJTklWJlkbNlTVEUkeneTiOY5/UpL/3Vr72qgPT1vbtGOu0Vq7\ntqr+IMlnquqEJH+e5H/MtEOYwX5Jbm+trR6P2arq5iT7Z7SzmMkrkxyc5Kxx/b/fWrttHr8XWEaO\nP/74XH755UmSSy+9dL3lBx54YO68885cddVVOeKII3LRRRflvvvuy8TExDpfoK+66qrcddddefGL\nXzyn8T/4wQ/m2c9+dp7+9KfPWJteCQyFXqlXApu3eYUNrbXjk6SqXp3k9EwJHFpr94wPAXtfVe2Y\n5CtJvpVk9ZShXpvRIWOrp7x/vfGr6qlJXprkyFnKm3bMKeNfVVUnJrkiyUdba6fOMmYXrbWPJ/n4\nYvwuYOmdd955SZJzzz03J5988npfonfZZZd8+tOfzimnnJL7778/hx9+eA4++OBsvfW6LXnVqlU5\n/vjj13t9uvFvuOGGXHDBBfnyl788W3l6JTAIeuXG0SuB5WKj7kbRWju3qj5aVbu11u6csuzyJJcn\naw/zuiOjwCHj13ZM8m+SHDaX8ZM8N6MjI743Tm/3SnJ2Ve3dWjtzrmOO13tMkvcmOS3JK6rqN1tr\nV8xj029JsndVbT3pcLf9k9w8jzGALcSrX/3qvOlNb8qdd9659jziNY466qgcddToQK4HH3wwe+21\nVw4++OC1y++///588pOfzNVXXz2n8a+88spMTEzkwAMPTJLccccdecMb3pDbb7997fp6JTBEeqVe\nCWye5nQ3iqp6VI3u6bvm+XFJ7kzy82nWnXz1mT9L8qUp5529Isl1rbXvzGX81tqZrbW9W2srW2sr\nk3w1yRvWBA0zjTlNXXsmuSzJR1prpyQ5JsnHquq35zAFSZLW2k8zOh3kVeOXXprk1g2dVwdsOe6+\n++78+Mf/ckeyCy+8MLvttlt23XX9C5lP/mJ76qmn5uijj84BBxyw9rXzzz8/hxxySJ785CfPafwT\nTzwxt99+eyYmJjIxMZFnPetZOfvss3PiiSdO/rV6JbDk9MoRvRLY3M31yIZdknyqqnbI6F6+/5jk\nxW18dcmq+liSi1prFyX58xrdhWLrjE6jeO2UsV6b5K/nM/4cTDfmVNsneU9r7VNJ0lr7dlW9IMmv\nTl2xqs7KaKexV5LPV9V9rbU1e7Y3Jjmnqv40yb1JTphjjcBm7p577snLX/7yPPDAA1mxYkV23333\nXHzxxVlzrZnXve51OfbYY3Psscfmne98Z6688sqsXr06hx9+eFatWrXOWKtWrcrrX//6eY0/B3ol\nsOT0Sr0S2DJs1N0oWHjuRtGfu1EsPHejGKb5XDV4uXGFdaAXvRJgdvPplXM6jQIAAABgroQNAAAA\nQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXW291AUATLXyHZcsdQmD\nNnHaMUtdAgAAbJAjGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAA\nALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6\nEjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2\nAAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAA\nAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAA\nXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0J\nGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsA\nAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAA\ngK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICu\nhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQN\nAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAA\nAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABA\nV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfC\nBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYA\nAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAA\noCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKAr\nYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2ED\nAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAA\nANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQ\nlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWw\nAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEA\nAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA\n6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK\n2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgA\nAAAAdCVsAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAA\nAHQlbAAAAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0\nJWwAAAAAuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVs\nAAAAALoSNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAA\nAAC6EjYAAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAA\nuhI2AAAAAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoS\nNgAAAABdCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYA\nAAAAXQkbAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAA\nAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABd\nCRsAAACAroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6ErYAAAAAHQlbAAAAAC6EjYAAAAAXQkb\nAAAAgK6EDQAAAEBXwgYAAACgK2EDAAAA0JWwAQAAAOhK2AAAAAB0JWwAAAAAuhI2AAAAAF0JGwAA\nAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdCVsAAAAALoSNgAAAABdCRsAAACA\nroQNAAAAQFfCBgAAAKArYQMAAADQlbABAAAA6GrrpS5gc1dVByY5N8ljktyT5DWttW8ubVUAw7KY\nvXLlOy5ZiGE3GxOnHdNlHPM8M3O8OHrN85D4XgksJ45sWHhnJTm7tfbEJKcnOWdpywEYJL0SYHZ6\nJbBsVGttqWvYbFXVHkluSrJra211VVWS25M8p7V205R1T0pyUpIdxo/txuuusWOS+xel8H7UvPCW\nW72JmhfL5Jp3b61tt5TFbIheqeZFsNzqTdS8WLbUXjkUy/EzMx3bMTyby7YMcTvm3CudRrGw9kty\ne2ttdZK01lpV3Zxk/4x2Fmu11j6Y5IMzDVRVt7bW9l3IYntT88JbbvUmal4sy6xmvVLNC2q51Zuo\nebEss5q79cqhWGbzPyPbMTyby7Ys9+1wGgUAAADQlbBhYd2SZO+q2jpJxoe77Z/k5iWtCmBY9EqA\n2emVwLIibFhArbWfJvl6kleNX3ppklunnlc3R4M/FG4aal54y63eRM2LZdnUrFeqeREst3oTNS+W\nZVNz5145FMtm/mdhO4Znc9mWZb0dLhC5wKrqSRldKXi3JPcmOaG1dv2SFgUwMHolwOz0SmA5ETYA\nAAAAXTmNAgAAAOhK2DBAVbWiqv6qqr5fVTdV1Vs2sO4VVfXDqrp2/PiTRa71wKq6qqpurKqrq+op\nM6z34qr6TlV9r6r+W1XtvJh1Tqpj1nqramVV/fOkOb22qp6wFPWO6/lQVU1UVauqQzew3lDmeNZ6\nBzjH21fVhePPxXVV9cWqOmCGdYcyz3OqeWhz3dty6Zd65cLTKxeeXsl8zbNH71FVnxt/Zm6oqiOn\nWeegqvpFVZ2xsJWv93u7bEdVvXf8d3FdVV1TVb+9SPVv8j6oqp45rvvGqvpSVe2zGLVPqW+TtqOq\nHltVn6+q71bVN6rqgqrafXG3ou93gqr6D7Pt95ZUa81jYI8kxye5LMlWSXZN8qMkT5lh3SuSHLeE\ntX4pyWvGP78sydXTrLNjkp8kefL4+YeTfGDA9a5McvdSfw4m1XNkkn2TTCQ5dIZ1hjTHc6l3aHO8\nfZIX5V9OLXtLkisGPs9zrXlQc70A87As+qVeuSg165ULX7Ne6THf+Z9Pj/6bJO8e/3xYkluTbDNp\n+TZJrkzyt0nOWI7bkeSFSXYY/3xIknuSPHIR6t+kfVBG/4P6piRHjZ+/PcmnluDztKnbsWeS50xa\n9wNJzllu2zFpnV9PcumG9iNL/VjyAjym+ZeSXJLk3056/v4k75lh3SuydF+e98jo4kRbj59XkjuS\nHDBlvZcn+dyk5wdndPXkoda7MgP8wrGhRjKUOZ5HvYOc40n1PSPJxHKY5znUPOi57rDdg++XeuWi\n169XLl7teqXHbPM9nx59f5K9Jj3/P0n+1aTnpyb5oyTvzuKHDd22Y9LrK8a9duUC177J+6CMQpPv\nTFq2U5JfJtl+Ef8ddN+XZvQf+lcs8mepy3YkecT4s7XfhvYjS/1wGsUw7Z9RYrrGxPi1mby/qq6v\nqvOr6vELWtm69ktye2ttdZK00Sf/5qxf63Tbs/Y+0YtorvUmySOr6mtV9fWqemdVbbWYhW6Eoczx\nfAx5jv84yWemeX3I8zxTzcmw53pTLYd+qVcOx1DmeD6GPMd6JbOZU4+uqt0y+r//d0y3blU9M8nh\nSf5qoQqdRZftmOKEJD+YMu5C6LEPWmdZa+2+jP6D+bELV/Z6uu5Lx3/fb8nM/WCh9NqO9yc5s7V2\ny8KWu2mG0PS3OFX1lSQHzrD41+Y53B+01m6pqkry5iQXZ5R8sfFuT7JPa+2nVbVrkvOTvC2jP2r6\nGOwcV9WfJjkgyW8tdS1zNUvNg53rudAvB21Zf7aWicHOsV5J0r1Hz/Q7HpHkI0le1lproxbe12Js\nx5Tf91tJ3pXkeeP/2GQRjb8HfCTJXUn+0xKXM29V9bwkj2utzXjtkKFwZMMSaK0d3lp7zAyPWzJK\ntx436S0rx69NN9Yt43+21tqHkzx+nKouhlsyKWEb/+HuP02t023P2kRvEc2p3tbag621n45//nlG\n5949d5Frna+hzPGcDHWOq+rtSX43yQtba7+YZpXBzfNsNQ91rudqM+mXeuVwDGWO52Soc6xXskav\nHt1auzPJ6qraa5p1n5BRD7q8qiaSvDXJH1bVuctsO5IkVfUbST6e5CWtte/22oYN6LEPWmdZVe2U\nZJckP164stfTc1/6oYyOMHhFa+3hBat4ej224+gkT6vRRYYnMrr+z6VV9ZIFrn3ehA3D9Kkkr6+q\nrcbp+isyStjXUVVbV9Wek56/NMlPxo1uwY13yl9P8qrxSy/N6Fyim6as+rmM/iCePH7+75L83WLU\nONlc663RVYS3Gf+8XUZfTv7vYta6EQYxx3M1xDmuqpOS/F5G/5fh7hlWG9Q8z6XmIc51Z4Pvl3rl\noAxijudqiHOsVzJPc+rRk9Z9U5JU1WFJ9knyP1tr17fWdm+trWytrUxyRpK/aa29euHLX6e2TdqO\n8fMjk/yXJP+6tXbdgledbvugryXZpqqOGj9/Y5LPttZ+uXCVr6vXvrSqPpTREU6/01p7aGGrXl+P\n7WitndJa22fS38StSV7UWvvsgm/AfLUBXDjCY91HRle6/c8Zncf1/SR/PGnZM5JcOv75kUmuSXJ9\nkusyukruIYtc65OSfCXJjeNafnX8+p8nedOk9Y5N8p2MrmR7YZJdlmhuZ603oy8YN4zn9JsZnR+4\n3RJ+Hs7KqImszuiqtDcNfI5nrXeAc7xvkjb+e7t2/PiHgc/znGoe2lwvwDwsi36pVy5KzXrlwtes\nV3rMd/7n1KPHz/dM8oUk3xv/OzhqhjHfncW/QGSX7Ri/9pNJn8Vr1/TXBa5/k/dBGV0z4xvjMa5I\nst8SfJ42aTuSPHvcD749af7/frltxzTjTWSgF4hccxsgAAAAgC6cRgEAAAB0JWwAAAAAuhI2AAAA\nAF0JGwAAAICuhA0AAABAV8IGAAAAoCthAwAAANCVsAEAAADoStgAAAAAdPX/AVdqdSt1loH+AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109b6208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.691781\n",
      "    -> metric: 0.527500\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.691781\n",
      "    -> metric: 0.527500\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.691781\n",
      "    -> metric: 0.527500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrEAAAKyCAYAAAByl/UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf0d5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,32,plot=True))\n",
    "network4.add(Dense(32,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train[:400],y_train[:400], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 9\n",
    "network5 = Network(loss=MSE(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network5.add(Dense(784,32,init='aaa'))\n",
    "network5.add(Dense(32,8,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.add(Dense(8,1,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network5.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adata_x = np.array([ [x] for x in np.arange(0., 2.0 * math.pi, 0.01) ])\n",
    "adata_y = np.vectorize(lambda x: math.sin(x))(adata_x)\n",
    "adata_x_train, adata_x_test, adata_y_train, adata_y_test = train_test_split(adata_x, adata_y, test_size=0.33, random_state=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65253919142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xfcb3b70>]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU2yLcY8uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfcb3908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "network8 = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[mean_absolute_error])\n",
    "network8.add(Dense(1,81,plot=True))\n",
    "network8.add(Dense(81,1,plot=True))\n",
    "network8.add(ReLU())\n",
    "network8.fit(adata_x_train,adata_y_train, epochs= 50, print_stats=False)\n",
    "print mean_absolute_error(network8.predict(adata_x_test), adata_y_test)\n",
    "\n",
    "plt.plot(adata_x, adata_y)\n",
    "plt.plot(adata_x, network8.predict(adata_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "(1L, 4L)\n",
      "[[-0.05424164  0.16972465 -0.41749995  0.39709858]]\n",
      "out_grad_4:\n",
      "(1L, 4L)\n",
      "[[-0.2019965  -0.23769518 -0.49487045  0.04320252]]\n",
      "out_grad_3:\n",
      "(1L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]]\n",
      "Testing d1...\n",
      "d1.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[-0.02707048 -0.20288487 -0.03403094]]\n",
      "d1.backward_pass(inp, out_grad_3):\n",
      "(1L, 4L)\n",
      "[[-0.01685764  0.21647157  0.04530229  0.03971692]]\n",
      "(5L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]\n",
      " [ 0.00132369 -0.00739713 -0.02593858]\n",
      " [-0.0041419   0.02314597  0.08116303]\n",
      " [ 0.01018852 -0.056936   -0.1996502 ]\n",
      " [-0.00969065  0.05415379  0.18989418]]\n",
      "Testing d2...\n",
      "d2.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[ 0.  0.  0.]]\n",
      "d2.backward_pass(inp, out_grad_3):\n",
      "(1L, 4L)\n",
      "[[ 0.  0.  0.  0.]]\n",
      "(5L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]\n",
      " [ 0.00132369 -0.00739713 -0.02593858]\n",
      " [-0.0041419   0.02314597  0.08116303]\n",
      " [ 0.01018852 -0.056936   -0.1996502 ]\n",
      " [-0.00969065  0.05415379  0.18989418]]\n",
      "Testing r...\n",
      "r.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.          0.16972465  0.          0.39709858]]\n",
      "r.backward_pass(inp, out_grad_4):\n",
      "(1L, 4L)\n",
      "[[-0.         -0.23769518 -0.          0.04320252]]\n",
      "None\n",
      "Testing s...\n",
      "s.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.5         0.73105858  0.5         0.73105858]]\n",
      "s.backward_pass(inp, out_grad_4):\n",
      "(1L, 4L)\n",
      "[[-0.05049912 -0.04673371 -0.12371761  0.00849413]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "d1 = Dense(input_size=4, output_size=3, init=\"gaussian\")\n",
    "d2 = Dense(input_size=4, output_size=3, init=\"zeros\")\n",
    "r = ReLU()\n",
    "s = Sigmoid()\n",
    "inp = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_4 = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_3 = np.random.random(3).reshape((1,-1)) - 0.5\n",
    "\n",
    "print \"inp:\"\n",
    "print inp.shape\n",
    "print inp\n",
    "print \"out_grad_4:\"\n",
    "print out_grad_4.shape\n",
    "print out_grad_4\n",
    "print \"out_grad_3:\"\n",
    "print out_grad_3.shape\n",
    "print out_grad_3\n",
    "\n",
    "print \"Testing d1...\"\n",
    "print \"d1.forward_pass(inp):\"\n",
    "t = d1.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d1.backward_pass(inp, out_grad_3):\"\n",
    "t = d1.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing d2...\"\n",
    "print \"d2.forward_pass(inp):\"\n",
    "t = d2.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d2.backward_pass(inp, out_grad_3):\"\n",
    "t = d2.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing r...\"\n",
    "print \"r.forward_pass(inp):\"\n",
    "t = r.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"r.backward_pass(inp, out_grad_4):\"\n",
    "t = r.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]\n",
    "\n",
    "print \"Testing s...\"\n",
    "print \"s.forward_pass(inp):\"\n",
    "t = s.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"s.backward_pass(inp, out_grad_4):\"\n",
    "t = s.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[0]:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[1]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[2]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[3]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[4]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[5]:\n",
      "[[ 0.29948146]\n",
      " [ 0.29947621]]\n",
      "out:\n",
      "[[ 0.57431575]\n",
      " [ 0.57431447]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "inps, out = n._forward_pass(inp)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention:  (2L, 3L)\n",
      "Attention:  (2L, 3L)\n",
      "Attention:  (2L, 4L)\n",
      "Attention:  (2L, 4L)\n",
      "Attention:  (2L, 1L)\n",
      "Attention:  (2L, 1L)\n",
      "kosmo test:,  (2L, 3L)\n",
      "output_grad.shape:  (2L, 1L)\n",
      "input_grad.shape:  (2L, 4L)\n",
      "tester:,  (2L, 4L)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matrix must be 2-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-394825b648e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlayer_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"inp:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-c0824ff16664>\u001b[0m in \u001b[0;36m_backward_pass\u001b[0;34m(self, inps, grad)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"kosmo test:, \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0minput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mweight_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mweights_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0minput_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-214e4e076819>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, input, output_grad)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"tester:, \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         return  np.asmatrix([ np.multiply( output_grad[i,:],   input[i,:]   )\n\u001b[0;32m---> 16\u001b[0;31m                           for i in xrange(input.shape[0])]), None    \n\u001b[0m",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.pyc\u001b[0m in \u001b[0;36masmatrix\u001b[0;34m(data, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmatrix_power\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\244976\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, data, dtype, copy)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matrix must be 2-dimensional\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matrix must be 2-dimensional"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "target = inp[:,0:1]\n",
    "inps, out = n._forward_pass(inp)\n",
    "grad = n.loss.backward_pass(out, target)\n",
    "layer_grads = n._backward_pass(inps, grad)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "print \"target:\"\n",
    "print target\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out\n",
    "print \"grad:\"\n",
    "print grad\n",
    "for i, grad in enumerate(layer_grads):\n",
    "    print \"layer_grads[\" + str(i) + \"]:\"\n",
    "    print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906  0.85913749\n",
      "  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "t:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864  0.221029\n",
      "  0.40498945  0.31609647  0.0766627   0.84322469]\n",
      "ce.forward_pass(y,t):\n",
      "0.736415962327\n",
      "ce.backward_pass(y,t):\n",
      "[-0.27490047 -0.08104869 -0.10469935  0.10054647 -0.24509895  0.5272741\n",
      "  0.11739401  0.0906406  -0.16913545 -0.05603779]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "ce = Crossentropy()\n",
    "y = np.random.random(10)\n",
    "t = np.random.random(10)\n",
    "\n",
    "print \"y:\"\n",
    "print y\n",
    "print \"t:\"\n",
    "print t\n",
    "print \"ce.forward_pass(y,t):\"\n",
    "print ce.forward_pass(y,t)\n",
    "print \"ce.backward_pass(y,t):\"\n",
    "print ce.backward_pass(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
      "grad2:\n",
      "[ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "grad3:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
      "opt.calculate_deltas(grad1):\n",
      "[-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
      "opt.calculate_deltas(grad2):\n",
      "[-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
      "opt.calculate_deltas(grad3):\n",
      "[-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "opt = Momentum(alpha=0.02, beta=0.99)\n",
    "grad1 = np.random.random(5)\n",
    "grad2 = np.random.random(5)\n",
    "grad3 = np.random.random(5)\n",
    "opt.calculate_deltas(grad1)\n",
    "opt.calculate_deltas(grad2)\n",
    "opt.calculate_deltas(grad3)\n",
    "\n",
    "print \"grad1:\"\n",
    "print grad1\n",
    "print \"grad2:\"\n",
    "print grad2\n",
    "print \"grad3:\"\n",
    "print grad3\n",
    "\n",
    "print \"opt.calculate_deltas(grad1):\"\n",
    "print opt.calculate_deltas(grad1)\n",
    "print \"opt.calculate_deltas(grad2):\"\n",
    "print opt.calculate_deltas(grad2)\n",
    "print \"opt.calculate_deltas(grad3):\"\n",
    "print opt.calculate_deltas(grad3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
