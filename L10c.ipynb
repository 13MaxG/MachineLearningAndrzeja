{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339, 784)\n",
      "(1850, 784)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(lambda x: x if x >= 0 else 0)(input))\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i,:],  \n",
    "            np.vectorize(lambda x: 1 if x >= 0 else 0)\n",
    "                (input[i,:])   ).tolist()[0] for i in xrange(input.shape[0])]), None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1. - 1./(1. + math.exp(gamma))\n",
    "    else:\n",
    "        return 1./(1. + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(sigmoid)(input))\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input_grad = np.array([ np.multiply( output_grad[i],  \n",
    "                np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) )\n",
    "                    (input[i,:])   ).tolist()[0]  for i in xrange(input.shape[0])])\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.choice([0.001, 1000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        return np.dot(input, self.weights)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        n = input.shape[1] \n",
    "        m = output_grad.shape[1]\n",
    "        input_grad = np.dot(output_grad, (self.weights[:-1,:]).T)\n",
    "        weight_grad=  np.array( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n",
    "                                   for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "        \n",
    "        return input_grad, weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(np.linalg.norm(delta_weights))\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        base = evaluate_loss()\n",
    "        grad = []\n",
    "        for (x, y), w in np.ndenumerate(self.weights):\n",
    "            self.weights[x, y] = w + 0.0001\n",
    "            changed = evaluate_loss()\n",
    "            grad.append(10000. * (changed - base))\n",
    "            self.weights[x, y] = w\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = []\n",
    "        self.ready = False\n",
    "    def calculate_deltas(self, grad):\n",
    "        if self.ready == False:\n",
    "            self.remember = grad\n",
    "            self.ready = True\n",
    "        \n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(-1.0*np.multiply(t, np.log(y))\n",
    "                          -np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        return ( -1.0*np.divide(t,y) + np.divide((1.0-y),(1.0-y)) )/ y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        times = 0\n",
    "        norms = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                times+=1\n",
    "                #norms = norms.append()\n",
    "        \n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        \n",
    "        nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "        maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "        plt.figure(figsize=(20,tmp), dpi=80)\n",
    "        plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "        iteri = 1\n",
    "        for i, layer in enumerate( self.layers):\n",
    "            if not layer.__class__.__name__ == \"Dense\":\n",
    "                continue\n",
    "            if layer.plot:\n",
    "                plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                iteri+=1\n",
    "                plt.title(\"Layer \"  +str(i))\n",
    "                plt.ylim([0, maximaxi])\n",
    "                plt.yscale('symlog')\n",
    "                plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "                \n",
    "                \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "\n",
    "        grad = self._join(layer_grads)\n",
    "        \n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, deltas):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "        \n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.230000\n",
      "    -> metric: 0.540000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.270000\n",
      "    -> metric: 0.460000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa776045650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network1.add(Dense(784,32))\n",
    "network1.add(Dense(32,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network2.add(Dense(784,32))\n",
    "network2.add(Dense(32,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Nie ma sensu ReLU jako ostatnia warstwa, ponieważ rozważany problem to problem klasyfikacji binarnej. Zwracane wartości to powinno być 0 lub 1, ew \"prawdopodobieństwo\". ReLU wyrzuca cokolwiek dodatniego. Być może dużego. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,32,plot=True))\n",
    "network4.add(Dense(32,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train[:400],y_train[:400], epochs= 3, print_stats=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ćwiczenie 9\n",
    "#network5 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "#network5.add(Dense(784,32,plot=True,init='aaa'))\n",
    "#network5.add(Dense(32,8,plot=True,init='aaa'))\n",
    "#network5.add(Sigmoid())\n",
    "#network5.add(Dense(8,1,plot=True,init='aaa'))\n",
    "#network5.add(Sigmoid())\n",
    "#network5.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "#print metric(network3.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adata_x = np.array([ [x] for x in np.arange(0., 2.0 * math.pi, 0.01) ])\n",
    "adata_y = np.vectorize(lambda x: math.sin(x))(adata_x)\n",
    "adata_x_train, adata_x_test, adata_y_train, adata_y_test = train_test_split(adata_x, adata_y, test_size=0.33, random_state=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 4\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 5\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 6\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 7\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 8\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 9\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 10\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAFyCAYAAAB7tlsfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naBGOAAC0CEcAAFr+D4vsbTfSMevmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa778be2a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65253919142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa775ae4c10>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naGFQSilVgxYGpZRSNWhhUEopVcP/A1RMAXshuLbJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa775ae47d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "network8 = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[mean_absolute_error])\n",
    "network8.add(Dense(1,81,plot=True))\n",
    "network8.add(Dense(81,1,plot=True))\n",
    "network8.add(ReLU())\n",
    "network8.fit(adata_x_train,adata_y_train, epochs= 10, print_stats=True)\n",
    "print mean_absolute_error(network8.predict(adata_x_test), adata_y_test)\n",
    "\n",
    "plt.plot(adata_x, adata_y)\n",
    "plt.plot(adata_x, network8.predict(adata_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
