{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339L, 784L)\n",
      "(1850L, 784L)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return np.linalg.norm(x)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None\n",
    "\n",
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(lambda x: x if x >= 0 else 0)(input))\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i,:],  np.vectorize(lambda x: 1 if x >= 0 else 0)(input[i,:])   ).tolist()[0] for i in xrange(len(input))]), None    \n",
    "        \n",
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(sigmoid)(input))\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input_grad = np.array([ np.multiply( output_grad[i],  \n",
    "                np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) )(input[i,:])   ).tolist()[0]  for i in xrange(input.shape[0])])\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.choice([0.001, 1000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        return np.dot(input, self.weights)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        n = input.shape[1] \n",
    "        m = output_grad.shape[1]\n",
    "        input_grad = np.dot(output_grad, (self.weights[:-1,:]).T)\n",
    "        weight_grad=  np.array( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) )  for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "        \n",
    "        return input_grad, weight_grad\n",
    "\n",
    "    def backward_pass_2(self, input, output_grad):\n",
    "        input_grad = np.dot(output_grad, self.weights.T)\n",
    "        weight_grad= np.dot(input.T, output_grad)\n",
    "    \n",
    "        return input_grad[:,-1:], weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(norm(delta_weights))\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        base = evaluate_loss()\n",
    "        grad = []\n",
    "        for (x, y), w in np.ndenumerate(self.weights):\n",
    "            self.weights[x, y] = w + 0.0001\n",
    "            changed = evaluate_loss()\n",
    "            grad.append(10000. * (changed - base))\n",
    "            self.weights[x, y] = w\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad\n",
    "\n",
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = []\n",
    "        self.ready = False\n",
    "    def calculate_deltas(self, grad):\n",
    "        if self.ready == False:\n",
    "            self.remember = grad\n",
    "            self.ready = True\n",
    "        \n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funkcje kosztu\n",
    "\n",
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(-1.0*np.multiply(t, np.log(y))-np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        return ( -1.0*np.divide(t,y) + np.divide((1.0-y),(1.0-y)) )/ y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class Network():\n",
    "\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        times = 0\n",
    "        norms = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                times+=1\n",
    "                #norms = norms.append()\n",
    "        \n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        \n",
    "        nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "        maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "        plt.figure(figsize=(20,tmp), dpi=80)\n",
    "        plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "        iteri = 1\n",
    "        for i, layer in enumerate( self.layers):\n",
    "            if not layer.__class__.__name__ == \"Dense\":\n",
    "                continue\n",
    "            if layer.plot:\n",
    "                plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                iteri+=1\n",
    "                plt.title(\"Layer \"  +str(i))\n",
    "                plt.ylim([0, maximaxi])\n",
    "                plt.yscale('symlog')\n",
    "                plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "                \n",
    "                \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "\n",
    "        grad = self._join(layer_grads)\n",
    "        \n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, deltas):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "        \n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-02e9592d6838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#jedna warstwa ukryta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e62ba6cd3123>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mbatch_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mtimes\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[1;31m#norms = norms.append()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e62ba6cd3123>\u001b[0m in \u001b[0;36m_fit_on_batch\u001b[0;34m(self, batch_X, batch_t)\u001b[0m\n\u001b[1;32m     74\u001b[0m         layer_grads = self._backward_pass(\n\u001b[1;32m     75\u001b[0m             \u001b[0minps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-e62ba6cd3123>\u001b[0m in \u001b[0;36m_backward_pass\u001b[0;34m(self, inps, grad)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0minput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mweight_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mweights_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mlayer_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0minput_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-4000bb2160d0>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, input, output_grad)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_grad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0minput_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mweight_grad\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m[\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 1.193870\n",
      "    -> metric: 0.540000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.675115\n",
      "    -> metric: 0.625000\n",
      "0.565945945946\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network1.add(Dense(784,32))\n",
    "network1.add(Dense(32,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.460000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.460000\n",
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network2.add(Dense(784,32))\n",
    "network2.add(Dense(32,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Nie ma sensu ReLU jako ostatnia warstwa, ponieważ rozważany problem to problem klasyfikacji binarnej. Zwracane wartości to powinno być 0 lub 1, ew \"prawdopodobieństwo\". ReLU wyrzuca cokolwiek dodatniego. Być może dużego. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.693118\n",
      "    -> metric: 0.500000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.693035\n",
      "    -> metric: 0.500000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692961\n",
      "    -> metric: 0.500000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBQAAAFyCAYAAABFiol8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAHAxJREFUeJzt3X+05GV9H/D3Z1lYQBEKAUR+eIMQoyZHEiWaGqlolSgR\nUfFYW6MlMRKqjQbskeScVhoNBTUEjZVINUHbxHgsZlXAHxSh0BgtakGIP3CjV34ISpAfUleU7tM/\nZiDD7r1777M7984M+3qdM4c7M898v5/nYfYzd9/7/X6nWmsBAAAA6LFm0gUAAAAAs0egAAAAAHQT\nKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECbKequryq3jLpOpZSVYdU\n1YVV9YOq+oeqeldV7TLpuoAdwwz1yvdW1d9V1X1V9d8mXQ+wY5mFXllVD6+qy6rqu1V1d1XdWFV/\nXFW7Tro2Vp9AAR5CamCLkKCq1iS5MMn3kxyY5ElJjkryttWtEGDyFuuVQ19OckqSj61iSQBTZyu9\n8t4kv5Pk4NbaI5IcmeQXk/zhatbHdBAowAqqqp+rqkur6raququqPl9Vzxx5/sqqetNmrzlhmPju\nMrz/lGFafXtVfbuq3lxVa0fGt6r63ar6bJJ7krxogVKenuRxSU5prd3dWvt2kn+f5FXSZGDSpqhX\nprX2ztbap5LcvSKTBdhG09IrW2s/aa1d21r78cjDm5I8dtxzZvoJFGDlnZnkkCT7JflEkr+uqv2G\nz52b5DeHRxDc76Qkf95a+3FVPTbJpUn+NMn+GRxVcFySN262j5OSvCrJw5N8dIEajkjyzdbaP4w8\ndlWS3ZP8zHbMDWBcpqFXAky7qemVVfUXVfV/k9yS5IlJ3rqdc2MGCRRgBbXWrmutXdJa29hau7e1\ndnqSluQpwyH/PcmuSZ6bJFX1mCRHJzlv+Pxrkny8tfZXrbX7hkcWvDXJiZvt6o9ba19pAxsXKOUR\nSe7c7LE7Rp4DmJgp6pUAU2vaemVr7V9lEDockeQ9SW4Yy0SZKWuXHgJsq6o6JING/U+T7JXB4WCP\nyCBVzjAtfl8GSfBFSV6d5NLW2jeHmzg8ydFVNRoGrMmWYeC3lijl7uH+R/2TkecAJmaKeiXA1JrG\nXtlaa0muGR79cEEG1+liByJQgJX1X5LcleTI1tp3q6oyODKgRsa8J8nXq+rQDBLik0aeuzXJX7bW\nfmOJ/Wxa4vmrk/x0Ve3TWrt9+NiRSX6Y5PrlTQVgxUxLrwSYZtPcK3eOayjskJzyAOOxU1Xtutlt\nTZI9M7igzR1V9bAk/ymDQ8Me0FqbT3JJkg8n+UmSj488/e4kJ1TVS6pql6raqaoOq6pf7azvyiRf\nS/JHVbVHVT06yZuTvK+19qNtmC/Atpj2Xpnh63dNslOSNcMa123LZAG20VT3yqr6pap6dlXtXlVr\nqupJSd6U5OJtnTCzS6AA43Fako2b3Z6ZwVfqPDGD9PgrSW5OctMCrz83g6/beV9r7b77H2ytXZXk\n2Ul+a/ja2zM4P+7RPcW11jYleX6SfTO4cM6XklyR5N/1bAdgO011rxz69LCulyd52fDnr2/DdgC2\n1bT3yl2SnJHB75R3JflQBhdv/M3O7fAQUIPTXoBJqqrHJbkuyU+31lzQBmABeiXA0vRKVpNAASZs\n+L3A70uyS2vtpZOuB2Aa6ZUAS9MrWW1OeRiT4blEf1NVn62qt0y6HmZDVR2bwWFrT0hy6oTLgRWn\nV7It9Ep2NHol20KvZBIcoTAmVbVza+0nw58vTfLC1pqv4wMYoVcCLE2vBGaFIxTGZKTp75TkOxl8\nHR8AI/RKgKXplcCsWHagUFWfrqovV9XVVXVlVf3CEuNPrKpWVccP7+9aVeur6vqquqaqLqmqw0bG\nv7Oq5oevOWJ79r3M+Wxtf4cPDzG7vqquqqonLHOb/zLJV5PcOXpFVWDHoVcua5t6Jezg9MplbVOv\nBKbesk95qKq9Wmt3Dn9+YZLTW2tPXGTsXJK/TFJJzmqtra/Bdzo/M8knWmutql6b5ITW2jOGrzkq\nyTeT/K8kx7fWru7ddw0uQnJga+1bI4+tS3LA8DtZR8dubX+fSfKB1tr5VXVCkje21o6sqsdn8P2t\noz7ZWjtz5LVrMvje19Nba9duts9TkpySZLcku61Zs2a3Aw44YKElXNStd/2oa/yO5pF77jrpEtjB\nbdq0KWvWDLLajRs35u67787++++/4Nj77rsvd9xxR1pr2WOPPbLbbrultZZ7770369atS1Xlnnvu\nycaNG7PvvvsmSe69996sXbs2t912W/bee+/ssssuSZKbb775x0n21yuBWaBX6pXA9Lr55pt/3Fpb\nt5yxa5e70fsb79CeSRZMIoaN771J/m2SPxp5/Y+SXDwy9HNJ3jDy/BXD12/zvpM8PskFVXV8a+3a\nqto9yfoklyY5a7NtLri/qtovyZOTPGf40AVJ3lVVh7XWvpLkGQvtuKrWtdbuba1tqqofJNnib/6t\ntbOTnH3//YMOOqjddNNCXx27uLnTLuoav6OZP/PYSZcADzj//PNzzjnn5Oqrr97iuU2bNuU5z3lO\nzjrrrJx66ql5/etfn+OPP36LcV/4whdywgknZH5+/kGPz83NZf369TniiME/hFXVbXolMIv0yi3p\nlcAkVdVtyx277EBhuOEPJDl6ePd5iww7JcnftNa+uFA4MOJ1ST46zn231q6uql9P8tGqOjHJHyT5\nH621sxYav4iDk9xy/6Flw6MpbkhySJINW3ndcVX1mgxOI7mitfaNjn0CDyGveMUrctlllyVJLr74\n4gXHnH322Xna056WJz3pSVvd1jve8Y684AUvWPa+9UpgVuiVeiUw+7oChdbaK5Kkql6ZQTL7oAZc\nVT+X5MVJjtradqrq95McluRZ49r3yLjPVtXJSS5P8qettTcvdx/bo7X24QwOSQN2cB/4wAeSJO9/\n//vzxje+cYtflK+77rpccMEFueKKK7a6nTPOOCMbNmzIpZdeuux965XArNArt1qfXgnMhG36lofW\n2vuTHF1V+2z21NOTzCX5RlXNJ3lqkvOGjThJUlVvSPKiJM9trXVfsXYr+75/+z+V5IwkZyY5pqqe\n0bmLG5McUFVrh9urDFLkG3prBXZsr3zlK3PZZZfl9ttvf9DjV155Zebn53P44Ydnbm4un/vc5/Lq\nV78655577gNj3v72t+cjH/lIPvGJT2T33Xfv3rdeCcwKvRJgdi0rUKiqvarqUSP3j09ye5Lvj45r\nrZ3bWjugtTbXWpvL4DoJr26tnTt83SlJXpbk2Zudv7bd+x4+t38G57W9u7X2e0mOTfLeqjpmOfsa\nzuF7Sb6U5OXDh16c5KbW2tYOSwPInXfeme985zsP3F+/fn322Wef7L333g8ad/LJJ+eWW27J/Px8\n5ufn89SnPjXnnXdeTj55kL2effbZ+eAHP5hLLrkke+2113J3X3olMAv0Sr0SeOhY7ikPeyb5cFXt\nlmRTktuS/FobfkVEVb03ycdaax9bbANVdVAGF2n8ZpLLhtdXuLe19pTh8+/JoFE/MsmnquoHrbXD\nltr3ZnZN8pbhYWJprX21qn41yc8vUM9i+0uSk5KcPzw14+4kJy5znYAd2F133ZWXvOQl2bhxY9as\nWZN99903F1544QMX6XrVq16V4447Lscdd9yi27jpppty6qmn5tBDD83RRw9O7123bl0+//nPJ0lO\nOumkXHTRRbn11ltzzDHHZI899siGDRuSQUC8Xq8Epp1eqVcCDx3L/tpIxs+3PIyfb3lgR1VVN7fW\nDpp0HSvBlcuBcdErAZbW0yu36RoKAAAAwI5NoAAAAAB0EygAAAAA3QQKAAAAQDeBAgAAANBNoAAA\nAAB0EygAAAAA3QQKAAAAQDeBAgAAANBNoAAAAAB0EygAAAAA3QQKAAAAQDeBAgAAANBNoAAAAAB0\nEygAAAAA3QQKAAAAQDeBAgAAANBNoAAAAAB0EygAAAAA3dZOugCYRnOnXTTpEqbW/JnHTroEAABg\nCjhCAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtA\nAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAA\nAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgm\nUAAAAAC6rZ10AcCOae60iyZdwlSbP/PYSZcAAABb5QgFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACA\nbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIF\nAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAA\noJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtA\nAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAA\nAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgm\nUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAA\nAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6\nCRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQA\nAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACA\nbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbmsn\nXQAAjNvcaRdNuoSpNn/msZMuAQB4CHCEAgAAANBNoAAAAAB0EygAAAAA3QQKAAAAQDeBAgAAANBN\noAAAAAB0EygAAAAA3QQKAAAAQDeBAgAAANBNoAAAAAB0EygAAAAA3QQKAAAAQDeBAgAAANBNoAAA\nAAB0WzvpAgCA2TR32kWTLmFqzZ957KRLAIAV5wgFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIF\nAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAA\noJtAAQAAAOi2dtIFAACwsLnTLpp0CVNt/sxjJ10CwA7NEQoAAABAN4ECAAAA0E2gAAAAAHQTKAAA\nAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADd\nBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoA\nAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABA\nN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4EC\nAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA\n0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2g\nAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAA\nAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQT\nKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAA\nAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADd\nBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoA\nAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABA\nN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4EC\nAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA\n0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2g\nAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAA\nAHQTKAAAAADdBAoAAABAN4ECAAAA0E2gAAAAAHQTKAAAAADdBApjUFWHV9Vnq+r6qrqqqp4w6ZoA\npo1eCbA0vRKYJQKF8XhPkvNaaz+T5Kwk50+2HICppFcCLE2vBGZGtdYmXcNMq6r9kmxIsndr7b6q\nqiS3JPmV1tqGzcaekuSUJLsNb+uGY+/38CT3rErh46PmlTdr9SZqXi2jNe/bWls3yWK2Rq9U8yqY\ntXoTNa+WHbVXTotZfM8sxDymz0NlLtM4j2X3yrUrXckO4OAkt7TW7kuS1lqrqhuSHJLBB8IDWmtn\nJzl7sQ1V1U2ttYNWsthxU/PKm7V6EzWvlhmrWa9U84qatXoTNa+WGat5bL1yWszY+i/KPKbPQ2Uu\nsz4PpzwAAAAA3QQK2+/GJAdU1dokGR6adkiSGyZaFcB00SsBlqZXAjNFoLCdWmvfS/KlJC8fPvTi\nJDdtfp7bMk39YWsLUPPKm7V6EzWvlpmpWa9U8yqYtXoTNa+Wmal5zL1yWszM+i/BPKbPQ2UuMz0P\nF2Ucg6p6bAZX4N0nyd1JTmytXTvRogCmjF4JsDS9EpglAgUAAACgm1MeAAAAgG4ChQmqqjVV9SdV\n9fdVtaGqXruVsZdX1beq6urh7XdXsc7Dq+qzVXV9VV1VVU9YZNyvVdXXquobVfWRqnrEatW4WR1L\n1ltVc1X1/0bW8+qqeswk6h3W886qmq+qVlVHbGXctKzxkvVO4RrvWlXrh++La6rqkqo6bJGx07LO\ny6p52tZ63PTKlaFXrjy9cnXolZPV2aP3q6pPDt8z11XVUQuMeVxV/bCqzlnZyrfY71jmUVVnDP9c\nXFNVX6iqY1ap/u3+DKqqpwzrvr6qPlNVB65G7ZvVt13zqKpHVdWnqurrVfXlqrqgqvZd3VmM93eC\nqvqPS33uTVRrzW1CtySvSHJpkp2S7J3k20mesMjYy5McP6E6P5PkXw9/PiHJVQuMeXiS7yb52eH9\ndyV52xTXO5fkzkm/B0bqOSrJQUnmkxyxyJhpWuPl1Dtta7xrkuflH0/1em2Sy6d8nZdb81St9Qqs\ng145uXqn6r2lV65KzXqlW+/69/ToP0ty+vDnI5PclGTnked3TnJlkr9Ics4sziPJc5PsNvz5iUnu\nSvKwVah/uz6DMviH5g1Jjh7ef0OSD0/g/bS989g/ya+MjH1bkvNnbR4jY34pycVb+xyZ9G3iBezI\ntyQXJfkXI/ffmuQti4y9PBP4JTnJfhlcEGjt8H4luTXJYZuNe0mST47cf3wGVyWe1nrnMoW/VGyt\nWUzLGnfUO5VrPFLfk5PMz8I6L6PmqV7rMcxbr5xcvVP53tIrV7V2vdJtqfXu6dH3JHnkyP3/neSf\nj9x/c5LfSXJ6Vj9QGNs8Rh5fM+y1cytc+3Z/BmUQjHxt5Lk9kvwoya6r+P9g7J+lGfxl/vJVfi+N\nZR5Jdh++tw7e2ufIpG9OeZisQzJIP+83P3xsMW+tqmur6kNVdeiKVvaPDk5yS2vtviRpg3f3Ddmy\nzoXm8sD3KK+i5dabJA+rqi9W1Zeq6j9U1U6rWeg2mJY17jHNa/y6JB9d4PFpXufFak6me623l145\nfnrldJnmNdYrWcqyenRV7ZPBv+LfutDYqnpKkl9O8icrVegSxjKPzZyY5JubbXcljOMz6EHPtdZ+\nkMFfih+1cmVvYayfpcM/36/N4v1gpYxrHm9Ncm5r7caVLXf7TEPTf8iqqr9NcvgiT/9C5+Z+vbV2\nY1VVktckuTCDFIttc0uSA1tr36uqvZN8KMmpGfzBZTymdo2r6veTHJbkWZOuZbmWqHlq13o59Mqp\nNtPvrRkxtWusV5KMvUcvto/dk7w7yQmttTZo4eO1GvPYbH/PSvKmJM8e/oWSVTT8PeDdSe5I8o4J\nl9Otqp6d5NGttUWv5TEtHKGwglprv9xa+6lFbjdmkFQ9euQlc8PHFtrWjcP/ttbau5IcOkxIV9qN\nGUnKhn84D1mgzoXm8kAyt4qWVW9r7d7W2veGP38/g3Phnr7KtfaaljVelmld46p6Q5IXJXlua+2H\nCwyZunVequZpXevl0iv1yjGbljVelmldY72S+42rR7fWbk9yX1U9coGxj8mgB11WVfNJXp/kN6rq\n/TM2jyRJVf2zJH+e5Pmtta+Paw5bMY7PoAc9V1V7JNkzyXdWruwtjPOz9J0ZHCnw0tbaphWreGHj\nmMczk/xiDS7sO5/B9Xgurqrnr3Dt3QQKk/XhJL9VVTsNk/KXZpCWP0hVra2q/UfuvzjJd4cNbUUN\nP3i/lOTlw4denMG5PRs2G/rJDN70Pzu8/2+S/NVK17e55dZbg6vz7jz8eV0Gv4D8n9WsdRtMxRov\n1zSucVWdkuRlGfxrwZ2LDJuqdV5OzdO41mOmV46ZXjk9pnGN9Uo6LatHj4z97SSpqiOTHJjkf7bW\nrm2t7dtam2utzSU5J8mftdZeufLlP6i27ZrH8P5RSf5rkhe01q5Z8aozts+gLybZuaqOHt4/KcnH\nW2s/WrnKH2xcn6VV9c4MjlR6YWvtxytb9ZbGMY/W2u+11g4c+TNxU5LntdY+vuIT6NWm4EIOO+ot\ng6vI/ucMzq36+ySvG3nuyUkuHv78sCRfSHJtkmsyuALtE1exzscm+dsk1w/r+Pnh43+Q5LdHxh2X\n5GsZXCF2fZI9J7SuS9abwS8R1w3X8+8yOF9v3QTfC+/JoFHcl8HVXjdM+RovWe8UrvFBSdrwz9rV\nw9vnp3ydl1XztK31CqyDXjmheqftvaVXrkrNeqVb7/ovq0cP7++f5NNJvjH8f3D0Its8Pat/Ucax\nzGP42HdH3otX399fV7j+7f4MyuAaFl8ebuPyJAdP4P20XfNI8rRhP/jqyPr/9azNY4HtzWdKL8p4\n/9frAAAAACybUx4AAACAbgIFAAAAoJtAAQAAAOgmUAAAAAC6CRQAAACAbgIFAAAAoJtAAQAAAOgm\nUAAAAAC6CRQAAACAbv8f5Zcz/lsGXMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19328978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrEAAAKyCAYAAAByl/UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntayqw6QEkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x194f50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,32,plot=True))\n",
    "network4.add(Dense(32,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train[:400],y_train[:400], epochs= 3, print_stats=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of continuous and binary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-d9324a46f0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnetwork5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'aaa'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnetwork5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mnetwork5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-218-6d24fc6f6dd9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> loss: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> %s: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnDenses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Dense\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-02e9592d6838>\u001b[0m in \u001b[0;36mmetric\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 82\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of continuous and binary"
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 9\n",
    "network5 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network5.add(Dense(784,32,plot=True,init='aaa'))\n",
    "network5.add(Dense(32,8,plot=True,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.add(Dense(8,1,plot=True,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network3.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
