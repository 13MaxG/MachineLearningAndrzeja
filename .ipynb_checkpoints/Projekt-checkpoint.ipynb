{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opis testownych modelów\n",
    "W projekcie skupiam się na dobieraniu odpowiedniej konstrukcji sieci konwolucyjnej. Model ten został wybrany, ponieważ jego założenia idealnie pasują do postawionego problemu - nauczenia komputera odpowiadania na dane CIFAR10.\n",
    "\n",
    "Sieci konwolucyjne skupiają się na odnajdywaniu charakterystych cech danych wejściowych. Zakładają one że dane wejściowe są obrazami, niekoniecznie dwuwymiarowymi. Pierwsze ukryte wartwy często uczą się wykrywać krawędzie na podstawie samych pikseli, podczas gdy głębsze warstwy są już w stanie wykrywać kształty. \n",
    "\n",
    "Sieci konwolucyjne generalnie budowane są z warstw:\n",
    "- konwolucyjnych - dopasowujących filtr o danym rozmiarze do fragmentu obrazka. mnożone są wagi filtru przez poszczególne piksele\n",
    "- aktywacji - pozwalających na dodawanie nieliniowych zależności. Często używane jest ReLU które odrzuca wartości ujemne, pozostawiając dodanie\n",
    "- zmniejszania wymiaru - tzw. pooling. Mają na celu redukcje ilości parametrów oraz obliczeń, pomagając unikać overfitowania \n",
    "- gęstych - znanych z klasycznych sieci. uczą się odpowiadać na podstawie danych z konwolucji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane\n",
    "Wejście to obrazek RGB 32x32x3. Wyjście to to liczba naturalna z przedziału [0,9]. Dane pochodą z datasetu CIFAR10. Kazdy obrazek przedstawia jedną klasę z: samolotów, samochodów, ptaków, kotów, sarenek, psów, żab, koni, statów i ciężarówek.  \t\t\t\t\t\t\t\t\t\t\n",
    " \n",
    "Dla celów nauczania wyjście zostało przetransformowane to 10 wymiarowej przestrzeni, tak, że 1->[1,0,...,0], 5->[0,0,0,0,0,1,0,0,0,0], 9->[...,0,1]. Motywacją jest to, żeby sieć nie uważała, że samochód o klasie 1, może być czymś pomiędzy samolotem(0) a ptakiem(2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametry\n",
    "\n",
    "Na poniższych parametrach przeprowadzony został random search.\n",
    "- optimizer: \n",
    "    - adam: \n",
    "        - lr z przedziału (0.01, 0.1)\n",
    "        - beta_1, beta_2 z przedziału (0.9, 0.999)\n",
    "    - adadelta: \n",
    "        - lr z przedziału (0.01, 0.1)\n",
    "        - rho z przedziału (0.01, 0.99)\n",
    "    - adagrad: \n",
    "        - lr z przedziału (0.01, 0.1)\n",
    "- loss:\n",
    "    - categorical_crossentropy\n",
    "    - mean_squared_error\n",
    "    - mean_squared_error\n",
    "- warstwy gęste, każda o rozmairze z [32, 64, 128]\n",
    "    - 3\n",
    "    - 2\n",
    "    - 1\n",
    "- warstwy konwolucji, każda o rozmairze z [32, 64, 128] i kernel z [(2,2),(3,3),(5,5)]\n",
    "    - 3\n",
    "    - 2\n",
    "    - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pOptimizers = ['adam', 'adadelta', 'adagrad']\n",
    "pOptimizersLr = [0.01, 0.1]\n",
    "pOptimizersRho = [0.01, 0.99]\n",
    "pOptimizersBeta = [0.9, 0.99]\n",
    "pLosses = ['categorical_crossentropy', 'mean_squared_error', 'mean_squared_error']\n",
    "pnDense = [1,2,3]\n",
    "pDenseSize = [16, 32,64]\n",
    "pnConv = [2,4,6]\n",
    "pConvSize = [64,96,128]\n",
    "pConvKernel = [(2,2),(3,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta,Adam,Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numModels = 30\n",
    "modelsParam = [{\n",
    "    'optimizer':np.random.choice(pOptimizers),\n",
    "    'optimizer_lr': random.uniform(pOptimizersLr[0],pOptimizersLr[1]),\n",
    "    'optimizer_rho': random.uniform(pOptimizersRho[0],pOptimizersRho[1]),\n",
    "    'optimizer_beta1': random.uniform(pOptimizersBeta[0],pOptimizersBeta[1]),\n",
    "    'optimizer_beta2': random.uniform(pOptimizersBeta[0],pOptimizersBeta[1]),\n",
    "    'loss': np.random.choice(pLosses),\n",
    "    'n_dense': np.random.choice(pnDense),\n",
    "    'dense_size': np.random.choice(pDenseSize,max(pnDense)),\n",
    "    'n_conv': np.random.choice(pnConv),\n",
    "    'conv_size': np.random.choice(pConvSize,1),\n",
    "    'conv_kernel':[pConvKernel[x] for x in  np.random.choice(len(pConvKernel),max(pnConv))]\n",
    "} for i in range(numModels)]\n",
    "modelsAcc = [ 0 for i in range(numModels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(dic):\n",
    "    input_shape = (32, 32, 3)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(5, 5), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    conv_size = dic['conv_size'][0]\n",
    "    for i in range(dic['n_conv']):\n",
    "        model.add(Conv2D(conv_size, \n",
    "                         kernel_size=dic['conv_kernel'][i], \n",
    "                         padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(conv_size, \n",
    "                         kernel_size=dic['conv_kernel'][i], \n",
    "                         padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        if conv_size>16:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.25))\n",
    "        conv_size //= 2\n",
    "\n",
    "    model.add(Flatten())  \n",
    "\n",
    "    dense_size = dic['dense_size'][0]\n",
    "    for i in range(dic['n_dense']):\n",
    "        model.add(Dense(dense_size))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        dense_size //= 2\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = None\n",
    "    if dic['optimizer'] == 'adam':\n",
    "        optimizer = Adam(lr = dic['optimizer_lr'],\n",
    "                         beta_1 = dic['optimizer_beta1'],\n",
    "                         beta_2 = dic['optimizer_beta2'])\n",
    "    if dic['optimizer'] == 'adadelta':\n",
    "        optimizer = Adadelta(lr = dic['optimizer_lr'],\n",
    "                         rho = dic['optimizer_rho'])\n",
    "    if dic['optimizer'] == 'adagrad':\n",
    "        optimizer = Adagrad(lr = dic['optimizer_lr'])\n",
    "    model.compile(loss=dic['loss'],optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_labels(arr, filename):\n",
    "    pd_array = pd.DataFrame(arr)\n",
    "    pd_array.index.names = [\"Id\"]\n",
    "    pd_array.columns = [\"Prediction\"]\n",
    "    pd_array.to_csv(filename)\n",
    "\n",
    "def load_labels(filename):\n",
    "    return pd.read_csv(filename, index_col=0).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"E:\\\\Desktop\\\\dane_ml\\\\X_train.npy\")\n",
    "y_train = load_labels(\"E:\\\\Desktop\\\\dane_ml\\\\y_train.csv\")\n",
    "X_test = np.load(\"E:\\\\Desktop\\\\dane_ml\\\\X_test.npy\")\n",
    "\n",
    "# Mój keras ma kolorki na końcu\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, 32, 32).transpose(0,2,3,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 32, 32).transpose(0,2,3,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_X_train, first_X_test, first_y_train, first_y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=43)\n",
    "first_y_train = to_categorical(first_y_train, num_classes=10)\n",
    "first_y_test = to_categorical(first_y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model:  0\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "24320/45000 [===============>..............] - ETA: 31s - loss: 0.0909 - acc: 0.0998Pomijam!\n",
      "Testing model:  1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      " 3200/45000 [=>............................] - ETA: 120s - loss: 0.1682 - acc: 0.1056"
     ]
    }
   ],
   "source": [
    "for i,params in enumerate(modelsParam):\n",
    "    try:\n",
    "        print (\"Testing model: \", i)\n",
    "        model = buildModel(params)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config)\n",
    "        model.fit(first_X_train, first_y_train, batch_size=128, epochs=3, verbose=1, validation_data=(first_X_test, first_y_test))\n",
    "        score = model.evaluate(first_X_test, first_y_test, verbose=0)[1]\n",
    "\n",
    "        if score < 0.13:\n",
    "            print (\"Skipping this model\")\n",
    "            continue\n",
    "\n",
    "        model = buildModel(params)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config)\n",
    "        model.fit(first_X_train, first_y_train, batch_size=25, epochs=25, verbose=1, validation_data=(first_X_test, first_y_test))\n",
    "        score = model.evaluate(first_X_test, first_y_test, verbose=0) [1]\n",
    "        modelsAcc[i] = score\n",
    "    except:\n",
    "        print(\"Pomijam!\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13, 0, 0.13, 0.089999999999999997, 0.23999999999999999, 0.089999999999999997, 0, 0, 0.13, 0, 0, 0, 0, 0, 0, 0, 0.20000000000000001, 0, 0, 0, 0, 0, 0, 0.13, 0.25, 0, 0.070000000000000007, 0, 0, 0.13]\n"
     ]
    }
   ],
   "source": [
    "print(modelsAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13\n",
      "{'conv_size': array([96]), 'dense_size': array([16, 16, 16]), 'conv_kernel': [(3, 3), (2, 2), (2, 2), (3, 3), (3, 3), (3, 3)], 'n_conv': 2, 'optimizer_beta2': 0.921799746058887, 'optimizer_rho': 0.1667359243696495, 'loss': 'categorical_crossentropy', 'n_dense': 3, 'optimizer': 'adadelta', 'optimizer_lr': 0.02198045637255123, 'optimizer_beta1': 0.9815437201454797}\n"
     ]
    }
   ],
   "source": [
    "#selected model \n",
    "ind = np.argmax(modelsAcc[ind])\n",
    "print(modelsAcc[ind])\n",
    "print(modelsParam[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 62s - loss: 0.0907 - acc: 0.0985 - val_loss: 0.0900 - val_acc: 0.0950\n",
      "Epoch 2/100\n",
      "35375/45000 [======================>.......] - ETA: 11s - loss: 0.0900 - acc: 0.1022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-4fff632b0a24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_y_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_X_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\w\\programy\\conda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = buildModel(modelsParam[ind])\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model.fit(first_X_train, first_y_train, batch_size=125, epochs=100, verbose=1, validation_data=(first_X_test, first_y_test))\n",
    "score = model.evaluate(first_X_test, first_y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_labels([np.argmax(x)for x in model.predict(X_test)], \"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
