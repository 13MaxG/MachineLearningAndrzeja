{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L7: Klasyfikacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na dzisiejszych ćwiczeniach skupimy się na: \n",
    "\n",
    "1. Ewaluacji klasyfikatorów i modeli probabilistycznych na przykładzie rozpoznawania raka piersi\n",
    "2. Różnicach pomiędzy Naive Bayes z regresja logistyczną\n",
    "3. Praktycznym problemie klasyfikacji SPAMu (od wczytania danych do klasyfikatora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "E:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "E:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metryki (klasyfikator binarny)\n",
    "\n",
    "Przypomnijmy, że zadaniem uczenia maszynowego jest znalezienie modelu, który minimalizuje loss na zbiorze testowym (notebooki L5 oraz L6). Tutaj zajmujemy się definicjami metryk, z których każda także może być funkcją kosztu.\n",
    "\n",
    "<img width=300 src=\"wyklady2017/mum_figures/precision_recall.png\">\n",
    "\n",
    "Niech $y$ to prawdziwa klasa, a $\\hat{y}$ to predykcja. Najpopularniejsze metryki dla klasyfikatorów binarnych:\n",
    "\n",
    "* Accuracy\n",
    "\n",
    "$$ \\frac{TP + TN}{TN + FN + TP + FP} = p(\\hat{y} = y | x) $$\n",
    "\n",
    "* Precision \n",
    "\n",
    "$$ \\frac{TP}{TP + FP} = p(y=1| \\hat{y}=1) $$\n",
    "\n",
    "* Recall \n",
    "\n",
    "$$ \\frac{TP}{TP + FN} = p(\\hat{y}=1| y=1) $$\n",
    "\n",
    "## Imbalans klas\n",
    "\n",
    "Ref: https://svds.com/learning-imbalanced-classes/\n",
    "\n",
    "Chcemy zdefiniować metryki *zbalansowane*. Przez zbalansowaną metrykę rozumiemy metrykę jak najmniej wrażliwą na \"prior\" klas. [Wytłumaczyć czemu accuracy jest mylące]\n",
    "\n",
    "## Bardziej zaawansowane metryki\n",
    "\n",
    "Kolejne metryki (dla klasyfikatorów binarnych) będą oparte o confusion matrix:\n",
    "\n",
    "<img src=\"figures/L7/confusion_matrix.png\">\n",
    "\n",
    "### Balanced accuracy\n",
    "\n",
    "$$ \\frac{\\mbox{precision} + \\mbox{recall}}{2} $$\n",
    "\n",
    "Idea: Zbalansowana metryka, średnia precision i recall.\n",
    "\n",
    "Problemy:\n",
    "1. nie używa w ogóle TN. \n",
    "2. zwraca wysoki wynik dla danych typu duża przewaga pozytywnych klas i model zwracający zawsze pozytywną klasę (recall i precision wysokie)\n",
    "\n",
    "### F1\n",
    "\n",
    "$$ \\frac{\\mbox{precision} * \\mbox{recall}}{\\mbox{precision} + \\mbox{recall}} $$\n",
    "\n",
    "Idea: Zbalansowana metryka, średnia *harmoniczna* precision i recall.\n",
    "\n",
    "Problemy:\n",
    "1. nie używa w ogóle TN. \n",
    "2. zwraca wysoki wynik dla danych typu duża przewaga pozytywnych klas i model zwracający zawsze pozytywną klasę (recall i precision wysokie)\n",
    "\n",
    "### Matthews correlation coefficient\n",
    "\n",
    "$$ \\frac{TP*TN  - FP*FN}{\\sqrt{(TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)}} $$\n",
    "\n",
    "Idea: Zbalansowana metryka, która używa całej macierzy confusion.\n",
    "\n",
    "Plus: bardziej \"odporna\" na \"głupie\" klasyfikatory.\n",
    "\n",
    "## Funkcja kosztu\n",
    "\n",
    "Na podstawie confusion matrix możemy definiować funkcję kosztu. Ile płacimy za FN? W przypadku klasyfikacji raka, dużo \"tańsze\" jest skierowanie pacjenta na dodatkowe badania niż postawienie fałszywej negatywnej diagnozy!\n",
    "\n",
    "<img width=400 src=\"figures/L7/cost_mat.png\">\n",
    "\n",
    "Oczywiście zazwyczaj $C_{TP}$ oraz $C_{TN}$ jest 0. Funkcja kosztu 0-1 (albo accuracy) odtwarza $C_{FN} = C_{FP}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja wrażliwa na koszt\n",
    "\n",
    "Ref: http://web.cs.iastate.edu/~honavar/elkan.pdf\n",
    "\n",
    "Czasami jesteśmy bardziej zainteresowani w precision lub recall. Są to problemy ``cost-sensitive``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>0.372583</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "count  5.690000e+02  569.000000   569.000000    569.000000      569.000000   \n",
       "mean   3.037183e+07    0.372583    14.127292     19.289649       91.969033   \n",
       "std    1.250206e+08    0.483918     3.524049      4.301036       24.298981   \n",
       "min    8.670000e+03    0.000000     6.981000      9.710000       43.790000   \n",
       "25%    8.692180e+05    0.000000    11.700000     16.170000       75.170000   \n",
       "50%    9.060240e+05    0.000000    13.370000     18.840000       86.240000   \n",
       "75%    8.813129e+06    1.000000    15.780000     21.800000      104.100000   \n",
       "max    9.113205e+08    1.000000    28.110000     39.280000      188.500000   \n",
       "\n",
       "         area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "count   569.000000       569.000000        569.000000      569.000000   \n",
       "mean    654.889104         0.096360          0.104341        0.088799   \n",
       "std     351.914129         0.014064          0.052813        0.079720   \n",
       "min     143.500000         0.052630          0.019380        0.000000   \n",
       "25%     420.300000         0.086370          0.064920        0.029560   \n",
       "50%     551.100000         0.095870          0.092630        0.061540   \n",
       "75%     782.700000         0.105300          0.130400        0.130700   \n",
       "max    2501.000000         0.163400          0.345400        0.426800   \n",
       "\n",
       "       concave points_mean           ...             radius_worst  \\\n",
       "count           569.000000           ...               569.000000   \n",
       "mean              0.048919           ...                16.269190   \n",
       "std               0.038803           ...                 4.833242   \n",
       "min               0.000000           ...                 7.930000   \n",
       "25%               0.020310           ...                13.010000   \n",
       "50%               0.033500           ...                14.970000   \n",
       "75%               0.074000           ...                18.790000   \n",
       "max               0.201200           ...                36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/L7/breast_cancer_transformer.csv\")\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True)\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})\n",
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1166dae90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFVCAYAAADPM8ekAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11682e650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['diagnosis'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wybieramy cechy\n",
    "prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n",
    "train, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n",
    "train_X = train[prediction_var][0:100]\n",
    "train_y=train.diagnosis[0:100]\n",
    "test_X= test[prediction_var] \n",
    "test_y =test.diagnosis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912280701754\n",
      "0.910714285714\n",
      "0.83606557377\n",
      "0.189882234815\n"
     ]
    }
   ],
   "source": [
    "# Regresja logistyczna\n",
    "model = LogisticRegression(C=50)\n",
    "model.fit(train_X,train_y)\n",
    "pred = model.predict(test_X)\n",
    "print metrics.accuracy_score(pred, test_y)\n",
    "print metrics.precision_score(pred, test_y)\n",
    "print metrics.recall_score(pred, test_y)\n",
    "print metrics.log_loss(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 [3 pkt]\n",
    "\n",
    "1. Powyższy model ma istotnie wyższy recall niż precision. \n",
    "\n",
    "Zdefiniujmy jako model probabilistyczny model, który zwraca p($\\hat{y}$ | y). Obiekt LogisticRegression zwraca tą wartość funkcją ``predict_proba``\n",
    "\n",
    "1. Każdy model probabilistyczny można użyć do stworzenia klasyfikora, która może mieć precision 100% lub recall 100% trywialnie, jak? \n",
    "\n",
    "2. Krzywa precision/recall jest obliczana licząc precision oraz recall modelu probabilistycznego dla różnych wartości precision. Zarysuj wykres precision/recall dla modelu powyżej.\n",
    "\n",
    "Powinno wyjść:\n",
    "\n",
    "<img src=\"figures/L7/prec_recall.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 [3 pkt]\n",
    "\n",
    "Założmy, że $C_{FP}$ = 1 i $C_{FN}$ = 10, co odpowiada sytuacji w której nie przejmujemy się postawieniem fałszywej pozytywnej diagnozy.\n",
    "\n",
    "Według http://web.cs.iastate.edu/~honavar/elkan.pdf wystarczy w takim wypadku dodać przykładom odpowiednią wage.\n",
    "\n",
    "a) Przetestuj pare wag klasy negatywnej przez podanie argumentu class_weight do LogisticRegression. Dla każdej wartości wagi narysuj dokładność (accuracy) oraz wynik metryki FN_aversive. Powinno wyjść:\n",
    "\n",
    "<img src=\"figures/L7/fn_aversive.png\">\n",
    "\n",
    "b) Równoważnym sposobem tworzenia \"cost-sensitive\" klasyfikatora z modelu probabilistycznego jest zmiana progu (patrz Zadanie 1). Znajdź taki próg, aby wynik klasyfikatora z tym progiem był równoważny argumentowi class_weight, który daje w punkcie a) najlepszy wynik.\n",
    "\n",
    "Podpowiedź: Jeśli 2 sprawia problem, przejrzyj załączoną publikację"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FN_aversive(y_true, y_pred):\n",
    "    FN = sum((y_true == 1) * (y_pred != y_true))\n",
    "    FP = sum((y_true == 0) * (y_pred != y_true))\n",
    "    return 10 * FN + FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metryki c.d. (model probabilistyczny)\n",
    "\n",
    "Metryki wcześniej wprowadzone zakładają na wejściu klasyfikator, albo model probabilistyczny z dobranym na twardo thresholdem. Trochę bardziej skomplikowanym modelem może być model probabilistyczny, który zwraca jedynie prawdopodobieństwo\n",
    "\n",
    "## Entropia krzyżowa/log loss (dla klasyfikacji binarnej)\n",
    "\n",
    "Entropia krzyżowa, inaczej log loss jest niczym innym jak dobrze nam znanym log likelihood modelu zastosowanym do modeli probabilistycznych (zwracających prawdopodobieństwo):\n",
    "\n",
    "$$ LL(\\hat y, y) = CE(\\hat y, y) = \\sum_{i=1}^{N} y \\log\\hat(y) $$.\n",
    "\n",
    "Entropia krzyżowa może być też bezpośrednio optymalizowana, w odróżnieniu od metryk typu accuracy, precision czy recall.\n",
    "\n",
    "Warto wspomnieć, że niektóre modele mają dobre accuracy, ale słaby log loss (np. Naive Bayes).\n",
    "\n",
    "## ROC\n",
    "\n",
    "Krzywa ROC tworzona jest podobnie jak krzywa precision recall, tylko tym razem dla każdego progu liczymy true positive rate (p oraz false positive rate.\n",
    "\n",
    "<img src=\"figures/L7/roc_curve.png\">\n",
    "\n",
    "Czasami chcemy opisać krzywą ROC jedną liczbą, z oczywistych względów musimy coś \"oszukać\" (tj. stracić jakąś informację). Popularny sposób to pole powierzni pod krzywą ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes vs Regresja Logistyczna \n",
    "\n",
    "Ref: https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyprowadzenie\n",
    "\n",
    "[Wyprowadzić na zajęciach]\n",
    "\n",
    "**Wyprowadzenie zaczynamy od zdefiniowania procesu generowania danych**\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Zakładamy niezależność cech\n",
    "\n",
    "$$ p(X | Y) = \\prod_i P(X_i | Y) $$\n",
    "\n",
    "### Regresja logistyczna (binarna)\n",
    "\n",
    "Zdefiniujmy ``odds`` jako\n",
    "\n",
    "$$ o = \\frac{p(Y=1 | x)}{p(Y=0| x)} $$\n",
    "\n",
    "Wtedy regresja logistyczna definiuje:\n",
    "\n",
    "$$ \\log(o) = \\sum \\theta_i x_i $$\n",
    "\n",
    "**Teraz liczymy likelihood**\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes to model generatywny (co jak dowiemy się niedługo ma wady). Użyjmy reguły Bayesa aby policzyć likelihood:\n",
    "\n",
    "$$ p(Y | X) = P(X | Y) P(Y)  = ( \\prod_i P(X_i | Y)) P(Y) $$\n",
    "\n",
    "### Regresja logistyczna\n",
    "\n",
    "Przekształcając $$ \\log(o) = \\sum \\theta_i x_i $$ otrzymujemy *bezpośrednio*, że $$ p(y | x) = \\mbox{sigmoid}(\\sum \\theta_i x_i) $$, gdzie $sigmoid(a) = \\frac{1}{1 + \\exp(-a)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Różnice pomiędzy Naive Bayes a regresją logistyczną\n",
    "\n",
    "### Niezależność cech\n",
    "\n",
    "Naive Bayes zakłada niezależność cech (brak korekty liniowych zależności). Mówiąc inaczej możemy \"wrzucić\" do regresji logistycznej skorelowane cechy i się nic nie stanie. W przypadku modelu Naive Bayes nazywamy ten problem \"double counting\".\n",
    "\n",
    "\n",
    "### Model dyskryminatywny vs generatywny\n",
    "\n",
    "Przez model generatywny rozumiem model, który optymalizuje łączne prawdopodobieństwo p(x, y).\n",
    "\n",
    "**Obserwacja 1.** Modelowanie p(x | y) lub p(x) nie jest bezpośrednio niezbędne do modelowania p(y | x). \n",
    "\n",
    "Obserwacja 1. mówi nam, że model generatywny wykonuje \"dodatkową\" pracę. W związku z tym czemu modele generatywne są aktywnie wykorzystywane w praktyce? Jest to po prostu kolejny sposób regularyzacji! Modelując p(x | y) można (niezbyt ściśle) powiedzieć, że modelujemy sposób w jaki funkcjonuje świat. Zainteresowanych odsyłamy do https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf oraz [TODO]\n",
    "\n",
    "Naive Bayes to *model generatywny*, a *regresja logistyczna* to model dyskryminatywny. W związku z tym należy oczekiwać, że w granicy danych regresja logistyczna będzie osiągać lepsze wyniki, ale może być różnie w przypadku małej ilości danych\n",
    "\n",
    "<img src=\"figures/L7/ng_plot.png\">\n",
    "\n",
    "(Obrazek za https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf).\n",
    "\n",
    "### Log loss\n",
    "\n",
    "Naive Bayes daje zbyt optymistyczne prawdopodobieństwa. Jest dobry w accuracy, ale zły w log lossie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918128654971\n",
      "0.818181818182\n",
      "0.964285714286\n",
      "0.203777398379\n"
     ]
    }
   ],
   "source": [
    "# Przykład słabego log lossu modelu Naive Bayes\n",
    "model = GaussianNB()\n",
    "model.fit(train_X,train_y)\n",
    "pred = model.predict(test_X)\n",
    "print metrics.accuracy_score(test_y, pred,)\n",
    "print metrics.precision_score(test_y, pred)\n",
    "print metrics.recall_score(test_y, pred)\n",
    "# Regresja logistyczna osiąga 0.26\n",
    "print metrics.log_loss(test_y, model.predict_proba(test_X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 [3 pkt]\n",
    "\n",
    "Naive Bayes jest często stosowany do problemów klasyfikacyjnych na tekście. W tym zadaniu zajmiemy się klasyfikacją SPAMU. Na wejściu zadany jest test wiadomości e-mail, etykietą jest 0 (prawdziwa wiadomość, \"HAM\") lub 1 (SPAM). \n",
    "\n",
    "Podstawowym problemem jest sposób reprezentacji tekstu. Podobnie jak w przypadku rozważanych funkcji bazowych na wcześniejszych zajęciach, modele wymagają stałowymiarowego wektoru. Proszę użyć klasy CountVectorizer z sklearn w celu przekształcenia wiadomości do reprezentacji wektorowej.\n",
    "\n",
    "1. Zastosuj transformację tekstu do reprezentacji bag of words\n",
    "2. Naucz model Naive Bayes (MultinomialNB) przewidywać SPAM\n",
    "3. Pokaż problem \"double counting\" w modelu Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Załadowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = pd.read_csv('data/L7/SMSSpamCollection/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksploracja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ham</th>\n",
       "      <th>count</th>\n",
       "      <td>4827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">spam</th>\n",
       "      <th>count</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        message\n",
       "label                                                          \n",
       "ham   count                                                4827\n",
       "      unique                                               4518\n",
       "      top                                Sorry, I'll call later\n",
       "      freq                                                   30\n",
       "spam  count                                                 747\n",
       "      unique                                                653\n",
       "      top     Please call our customer service representativ...\n",
       "      freq                                                    4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  length\n",
      "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
      "1   ham                      Ok lar... Joking wif u oni...      29\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
      "3   ham  U dun say so early hor... U c already then say...      49\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...      61\n"
     ]
    }
   ],
   "source": [
    "messages['length'] = messages['message'].map(lambda text: len(text))\n",
    "print messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc173978>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFJCAYAAAB3vj+vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x474bd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.length.plot(bins=20, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000000000C82CF98>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000000000CF98D68>], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFdCAYAAADv+X8iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neAAADETBAwBgoP8HaODZKJCxDBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4e4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetworzenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print len( messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (5574L, 8713L)\n",
      "number of non-zeros: 74169\n",
      "sparsity: 0.15%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "clean_messages = messages['message'].as_matrix()\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None) \n",
    "vectorizer.fit(clean_messages)\n",
    "data = vectorizer.transform(clean_messages).toarray()\n",
    "\n",
    "#print vectorizer.get_feature_names()\n",
    "#print data\n",
    "\n",
    "messages_bow = data\n",
    "print 'sparse matrix shape:', messages_bow.shape\n",
    "print 'number of non-zeros:', np.count_nonzero(messages_bow)\n",
    "print 'sparsity: %.2f%%' % (100.0 * np.count_nonzero(messages_bow) / (messages_bow.shape[0] * messages_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ..., 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "target = messages['label'].as_matrix()\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detector = MultinomialNB()\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size=0.33, random_state=43)\n",
    "spam_detector.fit(data_train, target_train)\n",
    "\n",
    "predictions = spam_detector.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.978260869565\n",
      "confusion matrix\n",
      "[[1556   23]\n",
      " [  17  244]]\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy', accuracy_score(predictions,target_test)\n",
    "print 'confusion matrix\\n', confusion_matrix(predictions,target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem double counting dla Naive Bayes w powyższym przypadku jest istotny. Polega on na tym, że Naive Bayses zakłada niezależność każdej ze współrzędnych. Jako, że nasze dane mają 8713 wymiarów, oraz pochodzą one z tekstu jesteśmy prawie pewni, że nie spełniają one założenia. Przykładowym problemem mogą być zwroty składające się zawsze z dwóch słów, \"good morning\". Będą one zaburzać niezależność, jeżeli słowa \"good\" lub \"morning\" pojawiały by się osobno.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
