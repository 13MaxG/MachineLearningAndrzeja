{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339L, 784L)\n",
      "(1850L, 784L)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(lambda x: x if x >= 0 else 0)(input))\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i,:],  \n",
    "            np.vectorize(lambda x: 1 if x >= 0 else 0)\n",
    "                (input[i,:])   ).tolist()[0] for i in xrange(input.shape[0])]), None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1. - 1./(1. + math.exp(gamma))\n",
    "    else:\n",
    "        return 1./(1. + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(sigmoid)(input))\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input_grad = np.array([ np.multiply( output_grad[i],  \n",
    "                np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) )\n",
    "                    (input[i,:])   ).tolist()[0]  for i in xrange(input.shape[0])])\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.choice([0.001, 1000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        return np.dot(input, self.weights)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        n = input.shape[1] \n",
    "        m = output_grad.shape[1]\n",
    "        input_grad = np.dot(output_grad, (self.weights[:-1,:]).T)\n",
    "        weight_grad=  np.array( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n",
    "                                   for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "        \n",
    "        return input_grad, weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(np.linalg.norm(delta_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = 0\n",
    "        self.ready = False\n",
    "    def calculate_deltas(self, grad):\n",
    "        #if self.ready == False:\n",
    "        #    self.remember = grad\n",
    "        #    self.ready = True\n",
    "        \n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(-1.0*np.multiply(t, np.log(y))\n",
    "                          -np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        return ( -1.0*np.divide(t,y) + np.divide((1.0-t),(1.0-y)) )/ (1.0 * y.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False,plot=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        times = 0\n",
    "        norms = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                times+=1\n",
    "                #norms = norms.append()\n",
    "        \n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        if plot:\n",
    "            nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "            maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "            plt.figure(figsize=(20,tmp), dpi=80)\n",
    "            plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            iteri = 1\n",
    "            for i, layer in enumerate( self.layers):\n",
    "                if not layer.__class__.__name__ == \"Dense\":\n",
    "                    continue\n",
    "                if layer.plot:\n",
    "                    plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                    iteri+=1\n",
    "                    plt.title(\"Layer \"  +str(i))\n",
    "                    plt.ylim([0, maximaxi])\n",
    "                    plt.yscale('symlog')\n",
    "                    plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "\n",
    "        grad = self._join(layer_grads)\n",
    "        \n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, deltas):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "        \n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.230000\n",
      "    -> metric: 0.540000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.270000\n",
      "    -> metric: 0.460000\n",
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "def metric2(y,t):\n",
    "    return mean_squared_error(np.around(y.flat), t)\n",
    "\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-849c0725f09d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-dc9c7c90dbc1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats, plot)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> loss: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    -> %s: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnDenses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Dense\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f7d49b8c00b6>\u001b[0m in \u001b[0;36mmetric2\u001b[0;34m(y, t)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmetric2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\metrics\\regression.pyc\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 231\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    232\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[1;32m    233\u001b[0m                                weights=sample_weight)\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\metrics\\regression.pyc\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\w\\Programy\\Conda\\envs\\python2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric2])\n",
    "network1.add(Dense(784,16))\n",
    "network1.add(Dense(16,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.460000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.460000\n",
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network2.add(Dense(784,32))\n",
    "network2.add(Dense(32,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Nie ma sensu ReLU jako ostatnia warstwa, ponieważ rozważany problem to problem klasyfikacji binarnej. Zwracane wartości to powinno być 0 lub 1, ew \"prawdopodobieństwo\". ReLU wyrzuca cokolwiek dodatniego. Być może dużego. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.693038\n",
      "    -> metric: 0.500000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692888\n",
      "    -> metric: 0.500000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692762\n",
      "    -> metric: 0.500000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBsAAAFyCAYAAAC0gdLxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108c5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.692851\n",
      "    -> metric: 0.527500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrEAAAKyCAYAAAByl/UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11071eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,32,plot=True))\n",
    "network4.add(Dense(32,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train[:400],y_train[:400], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.250000\n",
      "    -> metric: 0.500000\n",
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 9\n",
    "network5 = Network(loss=MSE(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network5.add(Dense(784,32,init='aaa'))\n",
    "network5.add(Dense(32,8,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.add(Dense(8,1,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network5.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adata_x = np.array([ [x] for x in np.arange(0., 2.0 * math.pi, 0.01) ])\n",
    "adata_y = np.vectorize(lambda x: math.sin(x))(adata_x)\n",
    "adata_x_train, adata_x_test, adata_y_train, adata_y_test = train_test_split(adata_x, adata_y, test_size=0.33, random_state=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 4\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "Epoch 5\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.245282\n",
      "    -> mean_absolute_error: 0.627722\n",
      "0.65253919142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xbc0deb8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU2yLcY8uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1092de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "network8 = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[mean_absolute_error])\n",
    "network8.add(Dense(1,81,plot=True))\n",
    "network8.add(Dense(81,1,plot=True))\n",
    "network8.add(ReLU())\n",
    "network8.fit(adata_x_train,adata_y_train, epochs= 5, print_stats=True)\n",
    "print mean_absolute_error(network8.predict(adata_x_test), adata_y_test)\n",
    "\n",
    "plt.plot(adata_x, adata_y)\n",
    "plt.plot(adata_x, network8.predict(adata_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "(1L, 4L)\n",
      "[[ 0.37638915  0.39460666 -0.41495579 -0.46094522]]\n",
      "out_grad_4:\n",
      "(1L, 4L)\n",
      "[[-0.33016958  0.3781425  -0.40165317 -0.07889237]]\n",
      "out_grad_3:\n",
      "(1L, 3L)\n",
      "[[ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing d1...\n",
      "d1.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[-0.18587636 -0.15772542  0.21099843]]\n",
      "d1.backward_pass(inp, out_grad_3):\n",
      "(1L, 4L)\n",
      "[[ 0.1555344  -0.22605305  0.20872542 -0.11524668]]\n",
      "(5L, 3L)\n",
      "[[ 0.17234465  0.01248305  0.07222046]\n",
      " [ 0.18068626  0.01308724  0.07571599]\n",
      " [-0.19000391 -0.01376213 -0.07962052]\n",
      " [-0.21106199 -0.01528738 -0.08844484]\n",
      " [ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing d2...\n",
      "d2.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[ 0.  0.  0.]]\n",
      "d2.backward_pass(inp, out_grad_3):\n",
      "(1L, 4L)\n",
      "[[ 0.  0.  0.  0.]]\n",
      "(5L, 3L)\n",
      "[[ 0.17234465  0.01248305  0.07222046]\n",
      " [ 0.18068626  0.01308724  0.07571599]\n",
      " [-0.19000391 -0.01376213 -0.07962052]\n",
      " [-0.21106199 -0.01528738 -0.08844484]\n",
      " [ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing r...\n",
      "r.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.37638915  0.39460666  0.          0.        ]]\n",
      "r.backward_pass(inp, out_grad_4):\n",
      "(1L,)\n",
      "[-0.33016958]\n",
      "None\n",
      "Testing s...\n",
      "s.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.59300192  0.59739117  0.39772441  0.38676162]]\n",
      "s.backward_pass(inp, out_grad_4):\n",
      "(1L,)\n",
      "[-0.07968664]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "d1 = Dense(input_size=4, output_size=3, init=\"gaussian\")\n",
    "d2 = Dense(input_size=4, output_size=3, init=\"zeros\")\n",
    "r = ReLU()\n",
    "s = Sigmoid()\n",
    "inp = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_4 = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_3 = np.random.random(3).reshape((1,-1)) - 0.5\n",
    "\n",
    "print \"inp:\"\n",
    "print inp.shape\n",
    "print inp\n",
    "print \"out_grad_4:\"\n",
    "print out_grad_4.shape\n",
    "print out_grad_4\n",
    "print \"out_grad_3:\"\n",
    "print out_grad_3.shape\n",
    "print out_grad_3\n",
    "\n",
    "print \"Testing d1...\"\n",
    "print \"d1.forward_pass(inp):\"\n",
    "t = d1.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d1.backward_pass(inp, out_grad_3):\"\n",
    "t = d1.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing d2...\"\n",
    "print \"d2.forward_pass(inp):\"\n",
    "t = d2.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d2.backward_pass(inp, out_grad_3):\"\n",
    "t = d2.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing r...\"\n",
    "print \"r.forward_pass(inp):\"\n",
    "t = r.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"r.backward_pass(inp, out_grad_4):\"\n",
    "t = r.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]\n",
    "\n",
    "print \"Testing s...\"\n",
    "print \"s.forward_pass(inp):\"\n",
    "t = s.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"s.backward_pass(inp, out_grad_4):\"\n",
    "t = s.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[0]:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[1]:\n",
      "[[-0.3155936   0.1508727  -0.03077691]\n",
      " [-0.35798308  0.05033805  0.21094002]]\n",
      "inps[2]:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "inps[3]:\n",
      "[[-0.0806043  -0.09601359  0.28344236 -0.27497282]\n",
      " [-0.0806043  -0.09601359  0.28344236 -0.27497282]]\n",
      "inps[4]:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "inps[5]:\n",
      "[[ 0.28846921]\n",
      " [ 0.28846921]]\n",
      "out:\n",
      "[[ 0.57162133]\n",
      " [ 0.57162133]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "inps, out = n._forward_pass(inp)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "target:\n",
      "[[-0.08080549]\n",
      " [-0.47261241]]\n",
      "inps[0]:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[1]:\n",
      "[[-0.3155936   0.1508727  -0.03077691]\n",
      " [-0.35798308  0.05033805  0.21094002]]\n",
      "inps[2]:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "inps[3]:\n",
      "[[-0.0806043  -0.09601359  0.28344236 -0.27497282]\n",
      " [-0.0806043  -0.09601359  0.28344236 -0.27497282]]\n",
      "inps[4]:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "inps[5]:\n",
      "[[ 0.28846921]\n",
      " [ 0.28846921]]\n",
      "out:\n",
      "[[ 0.57162133]\n",
      " [ 0.57162133]]\n",
      "grad:\n",
      "[[ 0.32621341]\n",
      " [ 0.52211687]]\n",
      "layer_grads[0]:\n",
      "[[ 0.          0.02097462  0.01587984]\n",
      " [ 0.         -0.01147535 -0.00572773]\n",
      " [ 0.          0.01071991  0.00277857]\n",
      " [ 0.         -0.01182592 -0.00197199]\n",
      " [ 0.         -0.06514896 -0.03360014]]\n",
      "layer_grads[1]:\n",
      "None\n",
      "layer_grads[2]:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.         -0.29870939  0.        ]]\n",
      "layer_grads[3]:\n",
      "None\n",
      "layer_grads[4]:\n",
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.84833028]]\n",
      "layer_grads[5]:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "target = inp[:,0:1]\n",
    "inps, out = n._forward_pass(inp)\n",
    "grad = n.loss.backward_pass(out, target)\n",
    "layer_grads = n._backward_pass(inps, grad)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "print \"target:\"\n",
    "print target\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out\n",
    "print \"grad:\"\n",
    "print grad\n",
    "for i, grad in enumerate(layer_grads):\n",
    "    print \"layer_grads[\" + str(i) + \"]:\"\n",
    "    print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906  0.85913749\n",
      "  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "t:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864  0.221029\n",
      "  0.40498945  0.31609647  0.0766627   0.84322469]\n",
      "ce.forward_pass(y,t):\n",
      "0.736415962327\n",
      "ce.backward_pass(y,t):\n",
      "[-0.27490047 -0.08104869 -0.10469935  0.10054647 -0.24509895  0.5272741\n",
      "  0.11739401  0.0906406  -0.16913545 -0.05603779]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "ce = Crossentropy()\n",
    "y = np.random.random(10)\n",
    "t = np.random.random(10)\n",
    "\n",
    "print \"y:\"\n",
    "print y\n",
    "print \"t:\"\n",
    "print t\n",
    "print \"ce.forward_pass(y,t):\"\n",
    "print ce.forward_pass(y,t)\n",
    "print \"ce.backward_pass(y,t):\"\n",
    "print ce.backward_pass(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
      "grad2:\n",
      "[ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "grad3:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
      "opt.calculate_deltas(grad1):\n",
      "[-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
      "opt.calculate_deltas(grad2):\n",
      "[-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
      "opt.calculate_deltas(grad3):\n",
      "[-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "opt = Momentum(alpha=0.02, beta=0.99)\n",
    "grad1 = np.random.random(5)\n",
    "grad2 = np.random.random(5)\n",
    "grad3 = np.random.random(5)\n",
    "opt.calculate_deltas(grad1)\n",
    "opt.calculate_deltas(grad2)\n",
    "opt.calculate_deltas(grad3)\n",
    "\n",
    "print \"grad1:\"\n",
    "print grad1\n",
    "print \"grad2:\"\n",
    "print grad2\n",
    "print \"grad3:\"\n",
    "print grad3\n",
    "\n",
    "print \"opt.calculate_deltas(grad1):\"\n",
    "print opt.calculate_deltas(grad1)\n",
    "print \"opt.calculate_deltas(grad2):\"\n",
    "print opt.calculate_deltas(grad2)\n",
    "print \"opt.calculate_deltas(grad3):\"\n",
    "print opt.calculate_deltas(grad3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
