{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339, 784)\n",
      "(1850, 784)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(lambda x: x if x >= 0 else 0)(input))\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i,:],  \n",
    "            np.vectorize(lambda x: 1 if x >= 0 else 0)\n",
    "                (input[i,:])   ).tolist()[0] for i in xrange(input.shape[0])]), None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1. - 1./(1. + math.exp(gamma))\n",
    "    else:\n",
    "        return 1./(1. + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        return np.asmatrix(np.vectorize(sigmoid)(input))\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input_grad = np.array([ np.multiply( output_grad[i],  \n",
    "                np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) )\n",
    "                    (input[i,:])   ).tolist()[0]  for i in xrange(input.shape[0])])\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.choice([0.001, 1000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        return np.dot(input, self.weights)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        input = np.append(input, np.array([[1] for i in input]), axis=1)\n",
    "        n = input.shape[1] \n",
    "        m = output_grad.shape[1]\n",
    "        input_grad = np.dot(output_grad, (self.weights[:-1,:]).T)\n",
    "        weight_grad=  np.array( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n",
    "                                   for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "        \n",
    "        return input_grad, weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(np.linalg.norm(delta_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = 0\n",
    "        self.ready = False\n",
    "    def calculate_deltas(self, grad):\n",
    "        #if self.ready == False:\n",
    "        #    self.remember = grad\n",
    "        #    self.ready = True\n",
    "        \n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(-1.0*np.multiply(t, np.log(y))\n",
    "                          -np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        return ( -1.0*np.divide(t,y) + np.divide((1.0-t),(1.0-y)) )/ y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False,plot=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        times = 0\n",
    "        norms = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                times+=1\n",
    "                #norms = norms.append()\n",
    "        \n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        if plot:\n",
    "            nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "            maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "            plt.figure(figsize=(20,tmp), dpi=80)\n",
    "            plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            iteri = 1\n",
    "            for i, layer in enumerate( self.layers):\n",
    "                if not layer.__class__.__name__ == \"Dense\":\n",
    "                    continue\n",
    "                if layer.plot:\n",
    "                    plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                    iteri+=1\n",
    "                    plt.title(\"Layer \"  +str(i))\n",
    "                    plt.ylim([0, maximaxi])\n",
    "                    plt.yscale('symlog')\n",
    "                    plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "\n",
    "        grad = self._join(layer_grads)\n",
    "        \n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, deltas):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "        \n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.230000\n",
      "    -> metric: 0.540000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.270000\n",
      "    -> metric: 0.460000\n",
      "0.482162162162\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-09c229ff1c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-dc9c7c90dbc1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, t, epochs, batch_size, print_stats, plot)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mbatch_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mtimes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;31m#norms = norms.append()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-dc9c7c90dbc1>\u001b[0m in \u001b[0;36m_fit_on_batch\u001b[0;34m(self, batch_X, batch_t)\u001b[0m\n\u001b[1;32m     75\u001b[0m         layer_grads = self._backward_pass(\n\u001b[1;32m     76\u001b[0m             \u001b[0minps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-dc9c7c90dbc1>\u001b[0m in \u001b[0;36m_backward_pass\u001b[0;34m(self, inps, grad)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0minput_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mweight_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mweights_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mlayer_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0minput_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-05668d0388b4>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(self, input, output_grad)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0minput_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         weight_grad=  np.array( [ [  np.sum( np.multiply(output_grad[:,b], input[:,a]) ) \n\u001b[0;32m---> 36\u001b[0;31m                                    for b in xrange(m)]  for a in xrange(n) ] )  \n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/matrixlib/defmatrix.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/matrixlib/defmatrix.pyc\u001b[0m in \u001b[0;36m__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network1.add(Dense(784,32))\n",
    "network1.add(Dense(32,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network2.add(Dense(784,32))\n",
    "network2.add(Dense(32,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train[:1000],y_train[:1000], epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Nie ma sensu ReLU jako ostatnia warstwa, ponieważ rozważany problem to problem klasyfikacji binarnej. Zwracane wartości to powinno być 0 lub 1, ew \"prawdopodobieństwo\". ReLU wyrzuca cokolwiek dodatniego. Być może dużego. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,32,plot=True))\n",
    "network4.add(Dense(32,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train[:400],y_train[:400], epochs= 3, print_stats=True,plot=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ćwiczenie 9\n",
    "network5 = Network(loss=MSE(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network5.add(Dense(784,32,init='aaa'))\n",
    "network5.add(Dense(32,8,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.add(Dense(8,1,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.fit(X_train[:100],y_train[:100], epochs= 3, print_stats=True)\n",
    "print metric(network5.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "adata_x = np.array([ [x] for x in np.arange(0., 2.0 * math.pi, 0.01) ])\n",
    "adata_y = np.vectorize(lambda x: math.sin(x))(adata_x)\n",
    "adata_x_train, adata_x_test, adata_y_train, adata_y_test = train_test_split(adata_x, adata_y, test_size=0.33, random_state=43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "network8 = Network(loss=Crossentropy(), optimizer=GD(learning_rate=0.05), metrics=[mean_absolute_error])\n",
    "network8.add(Dense(1,81,plot=True))\n",
    "network8.add(Dense(81,1,plot=True))\n",
    "network8.add(ReLU())\n",
    "network8.fit(adata_x_train,adata_y_train, epochs= 5, print_stats=True)\n",
    "print mean_absolute_error(network8.predict(adata_x_test), adata_y_test)\n",
    "\n",
    "plt.plot(adata_x, adata_y)\n",
    "plt.plot(adata_x, network8.predict(adata_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "(1, 4)\n",
      "[[ 0.37638915  0.39460666 -0.41495579 -0.46094522]]\n",
      "out_grad_4:\n",
      "(1, 4)\n",
      "[[-0.33016958  0.3781425  -0.40165317 -0.07889237]]\n",
      "out_grad_3:\n",
      "(1, 3)\n",
      "[[ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing d1...\n",
      "d1.forward_pass(inp):\n",
      "(1, 3)\n",
      "[[-0.18587636 -0.15772542  0.21099843]]\n",
      "d1.backward_pass(inp, out_grad_3):\n",
      "(1, 4)\n",
      "[[ 0.1555344  -0.22605305  0.20872542 -0.11524668]]\n",
      "(5, 3)\n",
      "[[ 0.17234465  0.01248305  0.07222046]\n",
      " [ 0.18068626  0.01308724  0.07571599]\n",
      " [-0.19000391 -0.01376213 -0.07962052]\n",
      " [-0.21106199 -0.01528738 -0.08844484]\n",
      " [ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing d2...\n",
      "d2.forward_pass(inp):\n",
      "(1, 3)\n",
      "[[ 0.  0.  0.]]\n",
      "d2.backward_pass(inp, out_grad_3):\n",
      "(1, 4)\n",
      "[[ 0.  0.  0.  0.]]\n",
      "(5, 3)\n",
      "[[ 0.17234465  0.01248305  0.07222046]\n",
      " [ 0.18068626  0.01308724  0.07571599]\n",
      " [-0.19000391 -0.01376213 -0.07962052]\n",
      " [-0.21106199 -0.01528738 -0.08844484]\n",
      " [ 0.45788953  0.03316528  0.19187711]]\n",
      "Testing r...\n",
      "r.forward_pass(inp):\n",
      "(1, 4)\n",
      "[[ 0.37638915  0.39460666  0.          0.        ]]\n",
      "r.backward_pass(inp, out_grad_4):\n",
      "(1,)\n",
      "[-0.33016958]\n",
      "None\n",
      "Testing s...\n",
      "s.forward_pass(inp):\n",
      "(1, 4)\n",
      "[[ 0.59300192  0.59739117  0.39772441  0.38676162]]\n",
      "s.backward_pass(inp, out_grad_4):\n",
      "(1,)\n",
      "[-0.07968664]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "d1 = Dense(input_size=4, output_size=3, init=\"gaussian\")\n",
    "d2 = Dense(input_size=4, output_size=3, init=\"zeros\")\n",
    "r = ReLU()\n",
    "s = Sigmoid()\n",
    "inp = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_4 = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_3 = np.random.random(3).reshape((1,-1)) - 0.5\n",
    "\n",
    "print \"inp:\"\n",
    "print inp.shape\n",
    "print inp\n",
    "print \"out_grad_4:\"\n",
    "print out_grad_4.shape\n",
    "print out_grad_4\n",
    "print \"out_grad_3:\"\n",
    "print out_grad_3.shape\n",
    "print out_grad_3\n",
    "\n",
    "print \"Testing d1...\"\n",
    "print \"d1.forward_pass(inp):\"\n",
    "t = d1.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d1.backward_pass(inp, out_grad_3):\"\n",
    "t = d1.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing d2...\"\n",
    "print \"d2.forward_pass(inp):\"\n",
    "t = d2.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d2.backward_pass(inp, out_grad_3):\"\n",
    "t = d2.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing r...\"\n",
    "print \"r.forward_pass(inp):\"\n",
    "t = r.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"r.backward_pass(inp, out_grad_4):\"\n",
    "t = r.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]\n",
    "\n",
    "print \"Testing s...\"\n",
    "print \"s.forward_pass(inp):\"\n",
    "t = s.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"s.backward_pass(inp, out_grad_4):\"\n",
    "t = s.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[0]:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[1]:\n",
      "[[-0.3155936   0.1508727  -0.03077691]\n",
      " [-0.35798308  0.05033805  0.21094002]]\n",
      "inps[2]:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "inps[3]:\n",
      "[[-0.0806043  -0.09601359  0.28344236 -0.27497282]\n",
      " [-0.0806043  -0.09601359  0.28344236 -0.27497282]]\n",
      "inps[4]:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "inps[5]:\n",
      "[[ 0.28846921]\n",
      " [ 0.28846921]]\n",
      "out:\n",
      "[[ 0.57162133]\n",
      " [ 0.57162133]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "inps, out = n._forward_pass(inp)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "target:\n",
      "[[-0.08080549]\n",
      " [-0.47261241]]\n",
      "inps[0]:\n",
      "[[-0.08080549  0.1852195  -0.29554775  0.37811744]\n",
      " [-0.47261241  0.17046751 -0.0826952   0.05868983]]\n",
      "inps[1]:\n",
      "[[-0.3155936   0.1508727  -0.03077691]\n",
      " [-0.35798308  0.05033805  0.21094002]]\n",
      "inps[2]:\n",
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "inps[3]:\n",
      "[[-0.0806043  -0.09601359  0.28344236 -0.27497282]\n",
      " [-0.0806043  -0.09601359  0.28344236 -0.27497282]]\n",
      "inps[4]:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "inps[5]:\n",
      "[[ 0.28846921]\n",
      " [ 0.28846921]]\n",
      "out:\n",
      "[[ 0.57162133]\n",
      " [ 0.57162133]]\n",
      "grad:\n",
      "[[ 0.32621341]\n",
      " [ 0.52211687]]\n",
      "layer_grads[0]:\n",
      "[[ 0.          0.02097462  0.01587984]\n",
      " [ 0.         -0.01147535 -0.00572773]\n",
      " [ 0.          0.01071991  0.00277857]\n",
      " [ 0.         -0.01182592 -0.00197199]\n",
      " [ 0.         -0.06514896 -0.03360014]]\n",
      "layer_grads[1]:\n",
      "None\n",
      "layer_grads[2]:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.         -0.29870939  0.        ]]\n",
      "layer_grads[3]:\n",
      "None\n",
      "layer_grads[4]:\n",
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.84833028]]\n",
      "layer_grads[5]:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "target = inp[:,0:1]\n",
    "inps, out = n._forward_pass(inp)\n",
    "grad = n.loss.backward_pass(out, target)\n",
    "layer_grads = n._backward_pass(inps, grad)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "print \"target:\"\n",
    "print target\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out\n",
    "print \"grad:\"\n",
    "print grad\n",
    "for i, grad in enumerate(layer_grads):\n",
    "    print \"layer_grads[\" + str(i) + \"]:\"\n",
    "    print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906  0.85913749\n",
      "  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "t:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864  0.221029\n",
      "  0.40498945  0.31609647  0.0766627   0.84322469]\n",
      "ce.forward_pass(y,t):\n",
      "0.736415962327\n",
      "ce.backward_pass(y,t):\n",
      "[-0.27490047 -0.08104869 -0.10469935  0.10054647 -0.24509895  0.5272741\n",
      "  0.11739401  0.0906406  -0.16913545 -0.05603779]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "ce = Crossentropy()\n",
    "y = np.random.random(10)\n",
    "t = np.random.random(10)\n",
    "\n",
    "print \"y:\"\n",
    "print y\n",
    "print \"t:\"\n",
    "print t\n",
    "print \"ce.forward_pass(y,t):\"\n",
    "print ce.forward_pass(y,t)\n",
    "print \"ce.backward_pass(y,t):\"\n",
    "print ce.backward_pass(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
      "grad2:\n",
      "[ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "grad3:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
      "opt.calculate_deltas(grad1):\n",
      "[-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
      "opt.calculate_deltas(grad2):\n",
      "[-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
      "opt.calculate_deltas(grad3):\n",
      "[-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "opt = Momentum(alpha=0.02, beta=0.99)\n",
    "grad1 = np.random.random(5)\n",
    "grad2 = np.random.random(5)\n",
    "grad3 = np.random.random(5)\n",
    "opt.calculate_deltas(grad1)\n",
    "opt.calculate_deltas(grad2)\n",
    "opt.calculate_deltas(grad3)\n",
    "\n",
    "print \"grad1:\"\n",
    "print grad1\n",
    "print \"grad2:\"\n",
    "print grad2\n",
    "print \"grad3:\"\n",
    "print grad3\n",
    "\n",
    "print \"opt.calculate_deltas(grad1):\"\n",
    "print opt.calculate_deltas(grad1)\n",
    "print \"opt.calculate_deltas(grad2):\"\n",
    "print opt.calculate_deltas(grad2)\n",
    "print \"opt.calculate_deltas(grad3):\"\n",
    "print opt.calculate_deltas(grad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
