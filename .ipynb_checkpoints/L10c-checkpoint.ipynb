{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339L, 784L)\n",
      "(1850L, 784L)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input[input<=0]=0\n",
    "        return input\n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input[input<=0]=0\n",
    "        input[input>0]=1\n",
    "        tmp = np.vstack([ np.multiply( output_grad[i,:],   input[i,:]   )\n",
    "                          for i in xrange(input.shape[0])])\n",
    "        return  np.asmatrix(tmp), None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def sigmoid(gamma):\n",
    "    if gamma < -15: \n",
    "        return 1. - 1./(1. + math.exp(-15))\n",
    "    if gamma > 15: \n",
    "        return 1./(1. + math.exp(-15.0))\n",
    "    \n",
    "    if gamma < 0:\n",
    "        return 1. - 1./(1. + math.exp(gamma))\n",
    "    else:\n",
    "        return 1./(1. + math.exp(-gamma))\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        f = np.vectorize(lambda x: sigmoid(x) , otypes=[np.float])\n",
    "        return f(input)\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        \n",
    "        f = np.vectorize(lambda x: sigmoid(x) * sigmoid(-x) , otypes=[np.float])\n",
    "        tmp = np.vstack([ np.multiply( output_grad[i],  \n",
    "                    f(input[i,:])   )  for i in xrange(input.shape[0])])\n",
    "    \n",
    "        input_grad = np.asmatrix(tmp)\n",
    "        weight_grad = None\n",
    "        return  input_grad, weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian', plot=False):\n",
    "        self.plot = plot\n",
    "        self.monitor = []\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "            \n",
    "        elif init == 'aaa':\n",
    "            self.weights = np.random.choice([0.0001, 1000000.01],  (input_size, output_size)  )\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        self.weights = np.asmatrix(self.weights)\n",
    "        \n",
    "    #Ćwiczenie 1 \n",
    "    def forward_pass(self, input):\n",
    "        input = np.append(np.array([[1] for i in input]), input, axis=1)\n",
    "        wynik = np.dot(input, self.weights)\n",
    "        return wynik\n",
    "    \n",
    "    #Ćwiczenie 1 \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        input = np.append(np.array([[1] for i in input]),input, axis=1)\n",
    "        input_grad = np.asmatrix(np.dot(output_grad, (self.weights[1:,:]).T))\n",
    "        weight_grad = np.asmatrix(np.dot(input.T,output_grad))\n",
    "        return input_grad, weight_grad\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        self.weights += delta_weights\n",
    "        self.monitor.append(np.linalg.norm(delta_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ćwiczenie 6\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.remember = 0\n",
    "    def calculate_deltas(self, grad):\n",
    "        self.remember = self.beta * self.remember + grad\n",
    "        return -self.alpha * self.remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        wynik = np.average(0.5 * np.square(y - t))\n",
    "        return wynik \n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "EPS = 0.000001\n",
    "#Ćwiczenie 5\n",
    "class Crossentropy(Loss):\n",
    "    def forward_pass(self, y, t):\n",
    "        wynik = np.average(-1.0*np.multiply(t, np.log(y))\n",
    "                          -np.multiply(1.0 - t, np.log( 1.0 - y )) )\n",
    "        return wynik\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        yy  = np.copy(y)\n",
    "        tt  = np.copy(t)\n",
    "        yy[yy<EPS]=EPS\n",
    "        #yy[yy>1.0-EPS]=1.0-EPS\n",
    "        tt[tt<EPS]=EPS\n",
    "        #tt[tt>1.0-EPS]=1.0-EPS\n",
    "        return (-np.divide(tt,yy)+np.divide((1-tt),(1-yy)))/ yy.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False,plot=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "        \n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "        if plot:\n",
    "            nDenses = sum([layer.__class__.__name__ == \"Dense\" for layer in self.layers])\n",
    "            maximaxi = np.max(np.max([layer.monitor  for layer in self.layers if (layer.__class__.__name__ == \"Dense\") and layer != None ]))\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            tmp =4 * int(math.ceil(nDenses / 5.0))\n",
    "            plt.figure(figsize=(20,tmp), dpi=80)\n",
    "            plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            iteri = 1\n",
    "            for i, layer in enumerate( self.layers):\n",
    "                if not layer.__class__.__name__ == \"Dense\":\n",
    "                    continue\n",
    "                if layer.plot:\n",
    "                    plt.subplot(int(math.ceil(nDenses / 5.0)) ,5,iteri)\n",
    "                    iteri+=1\n",
    "                    plt.title(\"Layer \"  +str(i))\n",
    "                    plt.ylim([0, maximaxi])\n",
    "                    plt.yscale('symlog')\n",
    "                    plt.bar( [x for x in range(len(layer.monitor))], layer.monitor)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        tempo = self.loss.backward_pass(out, batch_t)\n",
    "        layer_grads = self._backward_pass(inps, tempo)\n",
    "        grad = self._join(layer_grads)\n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        j = 0\n",
    "        for i in xrange(0, len(self.layers), 1):\n",
    "            if self.layers[i].__class__.__name__ == \"Dense\":\n",
    "                self.layers[i].update_weights(deltas[j])\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.array([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "    \n",
    "    #Ćwiczenie 2\n",
    "    def _forward_pass(self, X): \n",
    "        inps = []\n",
    "        output = None\n",
    "        for layer in self.layers:\n",
    "            inps.append(X)\n",
    "            tmp=layer.forward_pass(X)\n",
    "            X = tmp\n",
    "        return inps, X\n",
    "    \n",
    "    #Ćwiczenie 3\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [None for i in xrange(n)]\n",
    "        weight_grad = [None for i in xrange(n)]\n",
    "        layer_grads[n-1] = grad\n",
    "        for i in xrange(n-1, 0,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass((inps[i-1]), layer_grads[i]  ) \n",
    "            weight_grad[i-1] =  weights_grad\n",
    "            layer_grads[i-1] =  input_grad\n",
    " \n",
    "        return weight_grad\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 4\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "def metric(y,t):\n",
    "    return accuracy_score(np.around(y.flat), t)\n",
    "def metric2(y,t):\n",
    "    return mean_squared_error(np.around(y.flat), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.025611\n",
      "    -> metric: 0.955552\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.015795\n",
      "    -> metric: 0.964459\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.013211\n",
      "    -> metric: 0.968251\n",
      "Epoch 4\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.012001\n",
      "    -> metric: 0.970456\n",
      "Epoch 5\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.011259\n",
      "    -> metric: 0.972308\n",
      "0.977297297297\n"
     ]
    }
   ],
   "source": [
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.05), metrics=[metric])\n",
    "network.add(Dense(784,8))\n",
    "network.add(Dense(8,1)) #jedna warstwa ukryta\n",
    "network.add(Sigmoid())\n",
    "network.fit(X_train,y_train, epochs= 5, print_stats=True)\n",
    "print metric(network.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.358476\n",
      "    -> metric: 0.870888\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.249654\n",
      "    -> metric: 0.952112\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.156064\n",
      "    -> metric: 0.960138\n",
      "0.96013757827\n"
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 7:1\n",
    "network1 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.051), metrics=[metric])\n",
    "network1.add(Dense(784,5))\n",
    "network1.add(ReLU())\n",
    "network1.add(Dense(5,1))\n",
    "network1.add(Sigmoid())\n",
    "network1.fit(X_train,y_train, epochs= 3, print_stats=True)\n",
    "print metric(network1.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.000000\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: nan\n",
      "    -> metric: 0.000000\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 7:2\n",
    "network2 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.05,beta=0.25), metrics=[metric])\n",
    "network2.add(Dense(784,13))\n",
    "network2.add(ReLU())\n",
    "network2.add(Dense(13,1))\n",
    "network2.add(ReLU())\n",
    "network2.fit(X_train,y_train, epochs= 2, print_stats=True)\n",
    "print metric(network2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ćwiczenie 7:\n",
    "Ostatnia warstwa ReLU nie pasuje dobrze do problemy klasyfikacji, gdzie na wyjściu chcemy dostac informację zero jedynkową, lub \"procentową\".\n",
    "\n",
    "Ponadto powoduje ona problemy z loss Crossentropy dając NaN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.690046\n",
      "    -> metric: 0.478084\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.677257\n",
      "    -> metric: 0.523503\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.658492\n",
      "    -> metric: 0.574654\n",
      "Epoch 4\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.618366\n",
      "    -> metric: 0.849899\n",
      "Epoch 5\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.538764\n",
      "    -> metric: 0.915777\n",
      "Epoch 6\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.425359\n",
      "    -> metric: 0.942058\n",
      "Epoch 7\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.316180\n",
      "    -> metric: 0.951495\n",
      "Epoch 8\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.230719\n",
      "    -> metric: 0.958197\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBsAAAFyCAYAAAC0gdLxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAMTQAADE0B0s6tTgAAIABJREFUeJzt3XuwrWddH/DvLxxCwkUxaQiBk3DAxKAUE8VUxksAkUFF\nGQgUxopUGkqaqVPH1A7RsdUp1IIdseMwIqmpl7a2wRIuknpBlEubqqEkISgSgx5Dwkm4GIF4STzw\n9I+1Tthnn31Za+9nrfWutT6fmTXn7HV532e9e+3vevd3v+t5q7UWAAAAgF5OWvQAAAAAgNWibAAA\nAAC6UjYAAAAAXSkbAAAAgK6UDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAzVFXvrqpXL3oc\nu6mqc6rqHVX1uar6VFW9vqpOXvS4gPWwRFn581X1h1V1tKr+66LHA6yXZcjKqnp4Vf1uVd1dVZ+t\nqo9V1U9X1SmLHhvzp2yANVEjJxQIVXVSknck+Yskj03ylCQXJ/kP8x0hwOJtl5VjH0xyRZK3z3FI\nAIOzQ1bel+RfJDm7tfYlSS5K8rVJ/t08x8cwKBtgQarq71fVu6rqk1X1mar6/ar6lg23v6+qfmzT\nY144bopPHn/99eOW+9NV9edV9aqqOrDh/q2qfrCqrk9yb5JLthjKNyf5yiRXtNY+21r78yT/OsnL\ntdDAog0oK9Na+5nW2m8m+exMnizAHg0lK1trf9dau6W1dv+Gq7+Q5Pzez5nhUzbAYr0myTlJHpXk\n15O8paoeNb7tDUkuHR95cMxlSX6htXZ/VZ2f5F1Jfi7JmRkdjfDcJK/ctI7Lkrw8ycOTvG2LMVyY\n5E9ba5/acN0NSR6a5Cv28dwAehlCVgIM3WCysqr+W1X9VZIjSS5I8pP7fG4sIWUDLEhr7UOttXe2\n1v6mtXZfa+3Hk7QkXz++y/9MckqSb0+SqvryJM9IctX49n+e5Ndaa/+jtXZ0fETCTyZ52aZV/XRr\n7Y/ayN9sMZQvSfKXm667Z8NtAAszoKwEGKyhZWVr7XsyKiQuTPLGJLd3eaIslQO73wWYhao6J6MQ\n/4Ykj8zoELMvyaiNzrhlvjqjBvm6JK9I8q7W2p+OF3FekmdU1cai4KScWCL+2S5D+ex4/Rt92Ybb\nABZmQFkJMFhDzMrWWkty8/ioiTdnNC8Ya0TZAIvzn5J8JslFrbW7q6oyOqKgNtznjUk+UlVPyKhZ\nvmzDbXcl+ZXW2j/ZZT1f2OX2m5I8vqpOb619enzdRUn+Osmtkz0VgJkZSlYCDNmQs/LBMWfDWvIx\nCpi9B1XVKZsuJyX50owm17mnqh6W5N9ndLjZA1prh5O8M8mvJvm7JL+24eafTfLCqvqHVXVyVT2o\nqs6tqm+bcnzvS/LHSX6qqh5RVY9L8qokV7fW/nYPzxdgL4aelRk//pQkD0py0niMD9nLkwXYo0Fn\nZVX9g6p6VlU9tKpOqqqnJPmxJP9rr0+Y5aVsgNm7MsnfbLp8S0anBbogo9b5j5LcmeSOLR7/hoxO\nGXR1a+3osStbazckeVaSfzp+7Kcz+jze46YZXGvtC0m+K8kZGU3i84Ek703yr6ZZDsA+DTorx35r\nPK6XJPnu8f8/soflAOzV0LPy5CQ/kdE+5WeSXJPRRJKXTrkcVkCNPkoDDFVVfWWSDyV5fGvN5DoA\nW5CVALuTlcyTsgEGbHze46uTnNxae/GixwMwRLISYHeyknnzMYo5qKpvqqrfq6rrq+pfLno8LIeq\nek5Gh8I9KYnXDStPVrIXspJ1IyvZC1nJIjiyYQ6q6jFJPjU+5czvJnlOa+2vFz0ugCGRlQC7k5XA\nsnDqyzlorX18w5efj9NrAZxAVgLsTlYCy2Lij1FU1c9U1eGqalV14Q73+46q+kBV3VRVH6qqf7zF\nfV42Xs7zdlt+VZ0+Xtaxy61VdbSqThuf6uWt4+turqp3VtW502yAaZ9rVZ03Pmzt1qq6oaqeNMVy\nn5Xko04nCKtLVj5wm6wEtiUrH7hNVgIra+KPUVTVxUn+NMn/TvK81tpNW9ynMjpNytNbax+sqkNJ\n/jjJGa21z43vcyjJrySpJK9trb110uWP7/dDSZ7WWvuuGp3r+luS/HprrVXV9yd5YWvt6Vs87uQk\nj22t/dmG6x6S5KzxOWcneq5V9TtJfrm19otV9cIkr2ytXVRVX5XR+Wk3+o3W2mvGjzuY5JeTPLe1\ndu8W47siyRVJTk1y6kknnXTqWWedtdUmAAbsvvvuy4EDB/LJT34yp512Wk4++eQT7tNay5EjR3LG\nGWfkwQ9+cI4ePZq77747Z511Vk46adQBHz16NPfcc09aa3nEIx6RU089deLlJ8nnPve53H///Tn9\n9NNz55133p/k+ZGVwEDIygduG3RW3vWZv82jv/SUqR8HrK4777zz/tbaQya6c2ttqkuSw0ku3Oa2\nY2XDxeOvvzqj87SePP76pCS/neQpSd6dUeBOvPzx7R/e6nHj274uyeFtbrswyUeTPHn89UMzOl/2\nKyd9rkkeleSzSQ5seL53JTl3l232kPHzPn/S7fzYxz62AcvrcY97XLvxxhu3vO0LX/hCO+2009p7\n3vOe1lprN998c3vMYx7T7rvvvtZaa5///OfbM5/5zPb+97+/Pe1pT2tvectbplp+a6098YlPfOBx\nSe5oshIYIFk57Kx83CvfsafHAatrc1budOk6Z0NrrVXVi5NcW1V/leTLklzSWrt/fJcrkvyf1tr/\nGx0EMZ2q+obxMt+xzV1+IMnbthnbTVX1vUneVlUvS/Jvk/x2a+21Uwzh7CRHWmtHx8tsVXV7knOS\n3LbD4/5Rkq9K8sbx8/6e1tqdU6wXWCFVlWuuuSaXXHJJHvawh+Wee+7Jtdde+8Bf3l73utflG7/x\nG/OUpzxlT8u//vrrc8899+Q7v/M7t7uLrAQGT1ZuS1YCS6Fr2VBVB5L8aEYFw3ur6qIkb6+qJyd5\ndJIXJLl4H6u4NKNDzY5use4fSXJukmdu9+DW2vVVdXlGR1X8XGvtVfsYy8Raa7+Q5BfmsS5g+I4e\nPZpXv/rVufbaa3PxxRfnhhtuyHOf+9zccsstueuuu/LmN785733ve/e8/KuvvjovfelLc+DAiREv\nK4FlISu3Xa+sBJZC77NRXJjkMa219yZJa+2GqrojyddkFNiHkvzJuIV9dJKrquqs1tobdltwVT08\nyYuSXLTFbT+U5JIk39p2OPVPVf29JD+R5DVJXlxVT2+tvXuK5/exJGdV1YHW2tHxHBXnJLl9imUA\na+6mm27Kxz/+8Vx88ah7veiii3Lw4MHceOONue2223L48OGcd955SZK77rorr3jFK3LkyJFcfvnl\nuy773nvvzZve9KbccMMNJ9wmK4FlIisBltvEZ6OY0LHQ/MokGc/g++VJPtJae0Nr7azW2qHW2qEk\nv5fkFZMUDWMvTnJza+2PN145ngDnu5M8q7X2l9s9uKrOTPKuJD/bWvvhJM9J8vNV9exJn1xr7RNJ\nPpDkJeOrXpDRZ1Z2OtQN4Dhnn312jhw5kg9/+MNJkttuuy0f/ehHc/755+fyyy/PkSNHcvjw4Rw+\nfDhPfepTc9VVV02085wk11xzTS644II88YlPPO56WQksG1kJsNymOfXlG8dHKRxM8ptVdduG236+\nqp7bWrs7ySuSvKmqbk7yliTf31rbtaHdafljlya5etNjDib5qSSPTPK741MY/f42qzglyatba1cn\nSWvtw0m+LaMJfaYZy2VJLquqW5NcmeRluz03YH1cdtllOXjwYO644448+9nPzrnnfvGsaS9/+cvz\n9re/PWeeeWauuuqqvOhFL8oFF1yQ5z//+Xn961+fc845Z1/LT0aHBV966aWbH/agyEpgQGTlF4ca\nWQmsqIlPfcl8HTx4sN1xxx2LHgawAqrqztbawUWPYxZkJdCLrDzRoSuvy+HXPGcGIwKW1TRZ2ftj\nFAAAwAo5dOV1ix4CsISUDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAAAAB0pWwAAAAAulI2\nAAAAAF0pGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVsAAAAALpSNgAA\nAABdKRsAAACArpQNAAAAQFfKBgAAAKArZQMAAADQlbIBAAAA6ErZAAAAAHSlbAAAAAC6UjYAAAAA\nXSkbAAAAgK6UDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAAAAB0pWwAAAAAulI2AAAAAF0p\nGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVsAAAAALpSNgAAAABdKRsA\nAACArpQNAAAAQFfKBgAAAKArZQMAAADQlbIBAAAA6ErZAAAAAHSlbAAAAAC6UjYAAAAAXSkbAAAA\ngK6UDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAAAAB0pWwAAAAAulI2AAAAAF0pGwAAAICu\nlA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVsAAAAALpSNgAAAABdKRsAAACArpQN\nAAAAQFfKBgAAAKArZQMAAADQlbIBAAAA6ErZAAAAAHSlbAAAAAC6UjYAAABbOnTldYseArCklA0A\nAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAcx8SQwH4pGwAAAICulA0AAABAV8oGAAAAoCtl\nAwAAANCVsgEAAADoStkAAAAAdKVsAAAAduRUmMC0lA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADo\nStkAAAAAdKVsAAAAALpSNgAAAABdKRsAAACArpQNAAAAQFfKBgAAAKArZQMAAADQlbIBAAAA6ErZ\nAAAAAHSlbAAAAAC6UjYAAAAAXSkbAAAAgK6UDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAA\nAAB0pWwAAAAAulI2AAAAAF0pGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAJjaoSuvW/QQ\ngAFTNgAAAABdKRsAAACArpQNAAAAQFfKBgAAAKArZQMAAADQlbIBAAAA6ErZAAAAAHSlbAAAAAC6\nUjYAAAAAXSkbAAAAgK6UDQAAAEBXygYAAACgK2UDAAAA0JWyAQAAAOhK2QAAAAB0pWwAAAAAulI2\nAAAAAF0pGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVsAAAAujp05XWL\nHgKwYMoGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVsAAAAJmLiR2BSygYAAACgK2UDAAAA0JWy\nAQAA2DcfsQA2UjYAAABTUSwAu1E2AAAAe6Z4ALaibAAAAAC6UjYAAAAAXSkbAAAAYEVM+tGmWX8E\nStkAAAAAS2yIc6coGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAA9Lr7BIb\nlzPvM1YoGwCWxBBPaQQAwGR678sNfd9Q2QAAAABLYqeS4dhtm++ziGJC2QAAAABzsNsv/bMuBeZZ\nOigbAAAAYE3Mq3BQNgAAAMAALXKCx/1SNgAAAMBALFupsB1lAwAAAMzRfguFZTjiQdkAAAAAM9Sz\nXFgWygYAAADoYJGlwNAKCWUDAAAAzMl2pcDQyoL9UjYAAADAnK1aubCZsgEAAAAWZJrSYZkKCmUD\nAACwJ8v0iw/Mk58NZQMAAADsm4LheMoGAABgZvwCButJ2QAAAAA5sRzbqixToE1G2QAAAACdrXsp\noWwAAAAAulI2AAAAwJTW/ciF3SgbAAAAYAbWuZBQNgAAAIOzzr+kMQxeg/ujbAAGR7ADADBEzk4x\nOWUDAAAwMb9YgZ+DSSgbgKUj3AEA2K9Z7VPaVx1RNgAAALD2jpUEyoI+lA0AAACspEmKA/MwzIay\nAQAAgLWw1xJB+TA9ZQMAAADQlbIBAACAlbb5yATzM8yesgEAAICVokRYPGUDAAAA0JWyAVgqWmoA\nABg+ZQMAAADQlbIBAACAteFI2flQNgAAALCylAuLoWwA6MibGQAAKBuAPfALNQAAsBNlA7AnCgcA\nAI4Z0r7hkMayzpQNwCB5kwAAYD/sTy6WsgEAAIClpFAYLmUDMPOQ9iYAAEAv9i2Xg7IBWKhZvFl4\nAwIAWG3294ZP2QAsxKErrzvhTWLaN429vsks25vTso0XAGBWdtovss80LMoGYFuTBLZQ78v2BABg\nFSgbgJnZ7y/Oxx6/+d+e6wAAYDnZDxw2ZQPQ1bGPR6xy+C/yua3ydgUA2Av7R8OkbACOs6iw7j1f\nw6oXHgAAQ2Pfi42UDcDEekzIM0lJsFezfINb1mUDACwj+0fLT9kATKXHEQizPN2leR2AZbVdVskw\nAJaRsgHYs2XaAV6msQKrQ/YA9MlCebp8lA3A3Cz6TWLd1w+spp3O3LPTUV+waF6XsNqUDbCG1vnN\nfVkmwATYyuYiYa/ZMslj5RYwLz3mBWN4lA3AjlY14Kf5a99WfyUEmKee2bPXTJN/rBqvaZgtZQOs\nMW+yx1vE9nCIM7CdvUwYud8skUXMitcW0/B6WQ3KBlhzwnxklmexsI2BjaY9omre6wYYCpm13JQN\nsAb2crrKdQ737Z7/pNtknbcdcKLtPraw3ZFNiywjABbFvA2rR9kAK8DphIbBkRDApGb9c76X0tiO\nPox4vUMfygZYcXv5zC/TsS2BWXEGHQCWlbIBoAM75sBeDCE7hjAGYH3JoNWlbIA1sp/zsa+Lnocd\n97g/wCJNM3+EeW0A2EjZALAHTi8HbGfVf753e34muAQgUTbASjo2MZiduWGa5vsy7V8Mgflaxb/m\nz2rSYZMZA6wXZQMAwB44e8OJ9lKmbr5uXbcdsL7ZuaqUDQAz4g0TVsMkvwD7eYcT+bkg8TpYZ8oG\nAIApreu8BBuf016e3ypuEwC2pmwAGCg75TA8fi5HVnGuCmA6zsjFbpQNsOQ2B7cgB9g/WQqwf7vN\nbSNrV5uyAZbQrGb5ZrF8D2G5+Rk+3n4mi5zkfc72Bhg2ZQMsKW0wwGxMOy+BLN4f249pec2cyDZh\niJQNsAS8gQCwzPb6Pub9j2l4vcCwKBsAAOIXlWXhVKTsxvd/dmxbpqFsgIET6gCsA0c/wHrwM7s+\nlA2wRIQzwPzI3OXjewYwHMoGAGBtmWx3OfgewXLxM0uibAAAYMlNewYRAGZP2QAD5XziAACra9X3\n7Vb9+bE7ZQMMhEAGgL3zPjocvhfTsb1YVcoGGBiHggLMjlwF6EemshNlAwCw8rbaIVbuwvD52Rw+\n3yO2o2yAARDSAADAKlE2AAAAzJg/LrFulA2wQN50AGZnp7P6yF8AmC1lAyyYHV6A/dsuS2UswOzJ\nWraibAAAVoLCAQCGQ9kAM3RsB/fQldc9cAEAgGVlf5ZJKRtWjB/+4dhYNGx3GwD9yVggkQWwaMqG\nFSVcAVgH3u8AYJiUDTAFO7UAAKw6+7z0oGyADgQyAACz2ifcz3J7jMncY+yFsgH2wDwMAAB7Y58J\n1oOyATry5gkwf7IXoA9/UKMnZQPs0+YAFsgAAMC6UzbALnYqDxQLAPO102mFAYDhUDbABOzUAiyO\nDAYWTQ7B9JQNK0gYAgAAsEjKBtiG0gZg2OQ0wGQm+QiaTKU3ZQNMSAADzNZuOSuHgVlbx5xRQDAr\nygYYE6YA8zfpGX1kNAAsF2UDa2/jDqydWYDFk8WwPob6877VuGY91qFuC9grZQMrYb/hrHAAGCaZ\nDDCZveSl0wkzS8oGAGDhFvFXRGB4/NzPnm3MvCgbVpggOdGhK6+zXQBmTM4Cx/TKA3+Bh+WjbFhx\nk068tSy8YQEsj63egxzBAPQmQ2CYlA2sHDOZAwAwa8u+b7ns42f4lA2sBWEKAMC6cBQvQ6BsYKU4\nPBdg/vYzAzoAfclXhkLZsAbWJXBWbX4KAACAZaVsYBD2eqiXQgEAAHZnv5l5UzasqVmFzbxCTFgC\nDM9O2Sy3gaGTU9CXsmGNTDOfwX7DdtGPB2A+dstreQ4A60nZwMz13tG04woAADBsygamMskv+tPc\nZ5pDbpUMAMPlNGsAsyVfWTbKhjW0yHkVDl153Z4mgRSuAMtLhgNMR26yCpQNdNMrFIUrwPA42gzo\nQXbsnyPJWBbKBo6zyNASmADLQV4DTGfS3NzrUcAwRMoGtmViRwA2k+VAD6uYJUN4TkMYAxyjbFhj\n+wkjh28BAMDOpjn1PKwaZQNJjg+97QJwp4JBaAIMn6wGWD6ym2WlbGBXu5UPACy3SQpngFlblvxx\npjSYjLIBAJiInWuA/dlvjsphlomyAQDYkvl5gFUhx2D+lA0AsGIm2ane6T52yoFlJ8dg8ZQNALBG\ndpqHx845sGymyS0ZB/OlbACANWOHG6CfHplqQnZWkbIBgLWzbnMRrMvzBACGQ9kAAGvAX80AvmiW\n2SdXYUTZAAAA0NFuhYNCgnWgbACAJTHtxz/szAIAi6JsAAAAALpSNgDAEtvuaAdHNQDLSn7BalA2\nAMCcmawR4Iv2mn17eZychflRNgAAAGxBOQF7p2wAYC0NYQdyP7OVb3XboSuvG8TzAgBQNgCw1oby\ny7kzTAAMi7yF/VE2AMAm0x5RMIt12MkF1tk8shaYLWUDACyQHWEAYBUpGwBgF9OePWI/BcJ2czEA\nsHjOJgSTUzYAwDbsPAIA7I2yAQDyxWJh1gWDAgNgWOQyzIayAQC24OwQAKtJbsN8KBsAYMbmddQE\nAMBQKBsAYEqTlAZOXwkArDNlAwB0pGAAAFA2AMDEdioSlAwAAF+kbACAPVAuAAyPbIbhUDYAAABL\nRakAw6dsAIAd2KEFAJiesgEAAADoStkAAAAAdKVsAAAAALpSNgAAAABdKRsAAACArpQNAAAAQFfK\nBgAAAKArZcOMVdV5VXV9Vd1aVTdU1ZMWPSaAoZGVALuTlcAyUTbM3huTXNVa+4okr03yi4sdDsAg\nyUqA3clKYGlUa23RY1hZVfWoJLclOa21drSqKsmRJN/UWrtt032vSHJFklPHl4eM7zuNhye5d98D\n78NYtmYsJxrKOJLVHcsZrbWHdFpWd7LSWLZgLCcayjiS1R2LrDzeqn6f98tYtmYsJxrKOJIFZeWB\nTitka2cnOdJaO5okrbVWVbcnOSejN4sHtNZel+R1+1lZVd3RWju4n2X0YixbM5bhjiMxlgWSlQNg\nLFsbyliGMo7EWBZIVg6AsWzNWIY7jmRxY/ExCgAAAKArZcNsfSzJWVV1IEnGh7udk+T2hY4KYFhk\nJcDuZCWwVJQNM9Ra+0SSDyR5yfiqFyS5Y/Pn6jra1+FynRnL1ozlREMZR2IsCyErB8NYtjaUsQxl\nHImxLISsHAxj2ZqxnGgo40gWNBYTRM5YVZ2f0UzBpyf5bJKXtdZuWeigAAZGVgLsTlYCy0TZAAAA\nAHTlYxQAAABAV8qGFVBV51XV9VV1a1XdUFVPmvP6D1fVR6rqpvHlxePrH1VVv1FVf1JVH6qqizuv\n92fG625VdeGG67ddb1U9tKr+e1XdNt5eL5zxWN5dVX+2Ydv84BzGckpVvXW8zJur6p1Vde74trlu\nm13GMtdtU1W/VVUfHK/rfVX1NePrF/F62W4sc3+9rBNZKSs3jUVWbj0WWbnmZKWs3DQWWbn1WGTl\nJFprLkt+SfI7Sb5v/P8XJrlhzus/nOTCLa7/z0l+fPz/i5LckeTBHdd7cZKDm9e/03qT/Jskvzj+\n/+OTfCLJ6TMcy7uTPG+bx8xqLKck+Y588WNS35/k3YvYNruMZa7bJskjN/z/+UluXuDrZbuxzP31\nsk4XWSkrNy1XVm69TFm55hdZKSs3LVdWbr1MWTnJ2Hov0GW+lySPymiCoAPjryvJXUnOneMYjgvC\nDdffm+TRG77+gyTfOuv177TeJH+Y5KkbbntTkpfPcCw7/ZDPdCwblvt1SQ4vettsMZaFbZsk35fk\npoFsk41jWfjrZVUvslJWTjAmWXni8mXlml1kpaycYEyy8sTly8ptLj5GsfzOTnKktXY0Sdro1XJ7\nRuddnqf/UlW3VNXVVXVGVZ2eUYt314b7HJ71uCZY7zlJ/nyeY0ryk+Ntc01VPWHD9fMayw8kedtA\nts0PJHnbhq/num2q6per6mNJXpXkexe5TTaPZcNNi369rCpZucFA8mCzRb/2ZeWYrFxrsnKDgeTB\nZot+7cvKMVm5O2UDPVzcWntykq9N8qkkv7Tg8QzJ97bWviLJVyd5X5J3zHPlVfUjSc5N8sPzXO+E\nY5n7tmmtvbS1dnaSH03y2lmvbw9jWejrhZmTlduTlduPRVbKynUjK7cnK7cfi6wcYFYqG5bfx5Kc\nVVUHkqSqKqNW6vZ5DaC1dvv4379L8h+TfHNr7dNJjlbVozfc9dCsxzXBem9P8rh5jam19rHxv621\n9vokTxi3njMfS1X9UJJLknx7a+2vF7ltNo8lWey2aa39UpJnjL9c6Ovl2Fiq6vRFbpM1ICuPH4us\nHJOV25OVa0lWHj8WWTkmK7cnK7enbFhyrbVPJPlAkpeMr3pBkjtaa7fNY/1V9bCqeuSGq747yY3j\n//9qkn82vt9FSR6b5D1zGNZO69142+OTPD3JW2cxiKo6UFVnbvj6BUnuHofzTMdSVVdk9L14Vmvt\nLzfcNPdts9VY5r1tquqRVfWYDV8/L8mnk/xF5rxNdhjLZxf1elkHsnJLslJWbh6DrFxzsnJLslJW\nbh6DrJxU6zwJhMv8L0nOT/J/k9ya5P1JnjzHdT8hozeBDya5JaPPTR0a33Zmkt9K8icZTULyjM7r\nfmNGM7weTXJ3ktt2W2+ShyW5JslHx9vrRbMay3hd7x9vl5uTvCvJBXMYy8Ekbbzcm8aX31/Ettlu\nLPPeNhk1t3+wYX2/nfGESwvYJluOZVGvl3W6RFbKyuPHIitPHIesdElkpaw8fiyy8sRxyMoJL8dO\nGwIAAADQhY9RAAAAAF0pGwAAAICulA0AAABAV8oGAAAAoCtlAwAAANCVsgEAAADoStkAAAAAdKVs\nAAAAALophakCAAAADUlEQVRSNgAAAABd/X9CXw80sGi7ywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104cb748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.958918918919\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:1\n",
    "network3 = Network(loss=Crossentropy(), optimizer=Momentum(alpha=0.01,beta=0.05), metrics=[metric])\n",
    "network3.add(Dense(784,32,plot=True))\n",
    "network3.add(Dense(32,8,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.add(Dense(8,1,plot=True))\n",
    "network3.add(Sigmoid())\n",
    "network3.fit(X_train,y_train, epochs= 8, print_stats=True,plot=True)\n",
    "\n",
    "print metric(network3.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.124760\n",
      "    -> metric: 0.521916\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.124955\n",
      "    -> metric: 0.521916\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.124796\n",
      "    -> metric: 0.521916\n",
      "Epoch 4\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.124805\n",
      "    -> metric: 0.521916\n",
      "Epoch 5\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.124860\n",
      "    -> metric: 0.521916\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABrEAAAKyCAYAAAByl/UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAACAdP4fZvXdwq0vWQsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d6a8630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "# Ćwiczenie 8:2\n",
    "network4 = Network(loss=MSE(), optimizer=Momentum(alpha=0.25,beta=0.05), metrics=[metric])\n",
    "network4.add(Dense(784,8,plot=True))\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,8,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.add(Dense(8,1,plot=True))\n",
    "network4.add(Sigmoid())\n",
    "network4.fit(X_train,y_train, epochs= 5, print_stats=True,plot=True)\n",
    "print metric(network4.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.239042\n",
      "    -> metric: 0.521916\n",
      "Epoch 2\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.239042\n",
      "    -> metric: 0.521916\n",
      "Epoch 3\n",
      "    -> batch size: 256\n",
      "    -> loss: 0.239042\n",
      "    -> metric: 0.521916\n",
      "0.517837837838\n"
     ]
    }
   ],
   "source": [
    "#Ćwiczenie 9\n",
    "network5 = Network(loss=MSE(), optimizer=Momentum(alpha=0.05,beta=0.25), metrics=[metric])\n",
    "network5.add(Dense(784,32,init='aaa'))\n",
    "network5.add(Dense(32,8,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.add(Dense(8,1,init='aaa'))\n",
    "network5.add(Sigmoid())\n",
    "network5.fit(X_train,y_train, epochs= 3, print_stats=True)\n",
    "print metric(network5.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "(1L, 4L)\n",
      "[[-0.05424164  0.16972465 -0.41749995  0.39709858]]\n",
      "out_grad_4:\n",
      "(1L, 4L)\n",
      "[[-0.2019965  -0.23769518 -0.49487045  0.04320252]]\n",
      "out_grad_3:\n",
      "(1L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]]\n",
      "Testing d1...\n",
      "d1.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[-0.02707048 -0.20288487 -0.03403094]]\n",
      "d1.backward_pass(inp, out_grad_3):\n",
      "output_grad.shape:  (1L, 3L)\n",
      "input_grad.shape:  (1L, 4L)\n",
      "(1L, 4L)\n",
      "[[-0.01685764  0.21647157  0.04530229  0.03971692]]\n",
      "(5L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]\n",
      " [ 0.00132369 -0.00739713 -0.02593858]\n",
      " [-0.0041419   0.02314597  0.08116303]\n",
      " [ 0.01018852 -0.056936   -0.1996502 ]\n",
      " [-0.00969065  0.05415379  0.18989418]]\n",
      "Testing d2...\n",
      "d2.forward_pass(inp):\n",
      "(1L, 3L)\n",
      "[[ 0.  0.  0.]]\n",
      "d2.backward_pass(inp, out_grad_3):\n",
      "output_grad.shape:  (1L, 3L)\n",
      "input_grad.shape:  (1L, 4L)\n",
      "(1L, 4L)\n",
      "[[ 0.  0.  0.  0.]]\n",
      "(5L, 3L)\n",
      "[[-0.02440363  0.13637368  0.47820413]\n",
      " [ 0.00132369 -0.00739713 -0.02593858]\n",
      " [-0.0041419   0.02314597  0.08116303]\n",
      " [ 0.01018852 -0.056936   -0.1996502 ]\n",
      " [-0.00969065  0.05415379  0.18989418]]\n",
      "Testing r...\n",
      "r.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.          0.16972465  0.          0.39709858]]\n",
      "r.backward_pass(inp, out_grad_4):\n",
      "tester:,  (1L, 4L)\n",
      "Debuuje tutaj: [-0.         -0.23769518 -0.          0.04320252]\n",
      "(1L, 4L)\n",
      "[[-0.         -0.23769518 -0.          0.04320252]]\n",
      "None\n",
      "Testing s...\n",
      "s.forward_pass(inp):\n",
      "(1L, 4L)\n",
      "[[ 0.5         0.73105858  0.5         0.73105858]]\n",
      "s.backward_pass(inp, out_grad_4):\n",
      "Debuuje tutaj [array([-0.05049912, -0.04673371, -0.12371761,  0.00849413])]\n",
      "(1L, 4L)\n",
      "[[-0.05049912 -0.04673371 -0.12371761  0.00849413]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "d1 = Dense(input_size=4, output_size=3, init=\"gaussian\")\n",
    "d2 = Dense(input_size=4, output_size=3, init=\"zeros\")\n",
    "r = ReLU()\n",
    "s = Sigmoid()\n",
    "inp = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_4 = np.random.random(4).reshape((1,-1)) - 0.5\n",
    "out_grad_3 = np.random.random(3).reshape((1,-1)) - 0.5\n",
    "\n",
    "print \"inp:\"\n",
    "print inp.shape\n",
    "print inp\n",
    "print \"out_grad_4:\"\n",
    "print out_grad_4.shape\n",
    "print out_grad_4\n",
    "print \"out_grad_3:\"\n",
    "print out_grad_3.shape\n",
    "print out_grad_3\n",
    "\n",
    "print \"Testing d1...\"\n",
    "print \"d1.forward_pass(inp):\"\n",
    "t = d1.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d1.backward_pass(inp, out_grad_3):\"\n",
    "t = d1.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing d2...\"\n",
    "print \"d2.forward_pass(inp):\"\n",
    "t = d2.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"d2.backward_pass(inp, out_grad_3):\"\n",
    "t = d2.backward_pass(inp, out_grad_3)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1].shape\n",
    "print t[1]\n",
    "\n",
    "print \"Testing r...\"\n",
    "print \"r.forward_pass(inp):\"\n",
    "t = r.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"r.backward_pass(inp, out_grad_4):\"\n",
    "t = r.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]\n",
    "\n",
    "print \"Testing s...\"\n",
    "print \"s.forward_pass(inp):\"\n",
    "t = s.forward_pass(inp)\n",
    "print t.shape\n",
    "print t\n",
    "print \"s.backward_pass(inp, out_grad_4):\"\n",
    "t = s.backward_pass(inp, out_grad_4)\n",
    "print t[0].shape\n",
    "print t[0]\n",
    "print t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[0]:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[1]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[2]:\n",
      "[[ 0.06231216  0.          0.        ]\n",
      " [ 0.03836232  0.          0.        ]]\n",
      "inps[3]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[4]:\n",
      "[[ 0.          0.4840028   0.          0.        ]\n",
      " [ 0.          0.48391873  0.          0.        ]]\n",
      "inps[5]:\n",
      "[[ 0.29948146]\n",
      " [ 0.29947621]]\n",
      "out:\n",
      "[[ 0.57431575]\n",
      " [ 0.57431447]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "inps, out = n._forward_pass(inp)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kosmo test:,  (2L, 3L)\n",
      "inp:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "target:\n",
      "[[ 0.06066317]\n",
      " [ 0.17014516]]\n",
      "inps[0]:\n",
      "[[ 0.06066317 -0.08874451  0.22698799 -0.10080311]\n",
      " [ 0.17014516  0.20471561  0.10955987  0.04003446]]\n",
      "inps[1]:\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "inps[2]:\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "inps[3]:\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "inps[4]:\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "inps[5]:\n",
      "[[ 0.29948146]\n",
      " [ 0.29947621]]\n",
      "out:\n",
      "[[ 0.57431575]\n",
      " [ 0.57431447]]\n",
      "grad:\n",
      "[[ 0.25682629]\n",
      " [ 0.20208465]]\n",
      "layer_grads[0]:\n",
      "[[  1.00504857e-04   0.00000000e+00   0.00000000e+00]\n",
      " [  1.09423994e-05   0.00000000e+00   0.00000000e+00]\n",
      " [  4.06870903e-06   0.00000000e+00   0.00000000e+00]\n",
      " [  1.76162598e-05   0.00000000e+00   0.00000000e+00]\n",
      " [ -3.89801042e-06   0.00000000e+00   0.00000000e+00]]\n",
      "layer_grads[1]:\n",
      "None\n",
      "layer_grads[2]:\n",
      "[[ 0.          0.02863277  0.          0.        ]\n",
      " [ 0.          0.00148219  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "layer_grads[3]:\n",
      "None\n",
      "layer_grads[4]:\n",
      "[[ 0.45891094]\n",
      " [ 0.        ]\n",
      " [ 0.22209719]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "layer_grads[5]:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = Network(loss=MSE(), optimizer=GD(learning_rate=0.001), metrics=[])\n",
    "n.add(Dense(input_size=4, output_size=3, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=3, output_size=4, init=\"gaussian\"))\n",
    "n.add(ReLU())\n",
    "n.add(Dense(input_size=4, output_size=1, init=\"gaussian\"))\n",
    "n.add(Sigmoid())\n",
    "\n",
    "inp = np.random.random((2,4)) - 0.5\n",
    "target = inp[:,0:1]\n",
    "inps, out = n._forward_pass(inp)\n",
    "grad = n.loss.backward_pass(out, target)\n",
    "layer_grads = n._backward_pass(inps, grad)\n",
    "\n",
    "print \"inp:\"\n",
    "print inp\n",
    "print \"target:\"\n",
    "print target\n",
    "for i, inp in enumerate(inps):\n",
    "    print \"inps[\" + str(i) + \"]:\"\n",
    "    print inp\n",
    "print \"out:\"\n",
    "print out\n",
    "print \"grad:\"\n",
    "print grad\n",
    "for i, grad in enumerate(layer_grads):\n",
    "    print \"layer_grads[\" + str(i) + \"]:\"\n",
    "    print grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906  0.85913749\n",
      "  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "t:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864  0.221029\n",
      "  0.40498945  0.31609647  0.0766627   0.84322469]\n",
      "ce.forward_pass(y,t):\n",
      "0.736415962327\n",
      "ce.backward_pass(y,t):\n",
      "[-0.27490047 -0.08104869 -0.10469935  0.10054647 -0.24509895  0.5272741\n",
      "  0.11739401  0.0906406  -0.16913545 -0.05603779]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "ce = Crossentropy()\n",
    "y = np.random.random(10)\n",
    "t = np.random.random(10)\n",
    "\n",
    "print \"y:\"\n",
    "print y\n",
    "print \"t:\"\n",
    "print t\n",
    "print \"ce.forward_pass(y,t):\"\n",
    "print ce.forward_pass(y,t)\n",
    "print \"ce.backward_pass(y,t):\"\n",
    "print ce.backward_pass(y,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad1:\n",
      "[ 0.11505457  0.60906654  0.13339096  0.24058962  0.32713906]\n",
      "grad2:\n",
      "[ 0.85913749  0.66609021  0.54116221  0.02901382  0.7337483 ]\n",
      "grad3:\n",
      "[ 0.39495002  0.80204712  0.25442113  0.05688494  0.86664864]\n",
      "opt.calculate_deltas(grad1):\n",
      "[-0.02919466 -0.0529381  -0.0209018  -0.01117572 -0.04443381]\n",
      "opt.calculate_deltas(grad2):\n",
      "[-0.04608546 -0.06573052 -0.03151603 -0.01164424 -0.05866444]\n",
      "opt.calculate_deltas(grad3):\n",
      "[-0.05352361 -0.08111416 -0.03628929 -0.0126655  -0.07541077]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "opt = Momentum(alpha=0.02, beta=0.99)\n",
    "grad1 = np.random.random(5)\n",
    "grad2 = np.random.random(5)\n",
    "grad3 = np.random.random(5)\n",
    "opt.calculate_deltas(grad1)\n",
    "opt.calculate_deltas(grad2)\n",
    "opt.calculate_deltas(grad3)\n",
    "\n",
    "print \"grad1:\"\n",
    "print grad1\n",
    "print \"grad2:\"\n",
    "print grad2\n",
    "print \"grad3:\"\n",
    "print grad3\n",
    "\n",
    "print \"opt.calculate_deltas(grad1):\"\n",
    "print opt.calculate_deltas(grad1)\n",
    "print \"opt.calculate_deltas(grad2):\"\n",
    "print opt.calculate_deltas(grad2)\n",
    "print \"opt.calculate_deltas(grad3):\"\n",
    "print opt.calculate_deltas(grad3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
