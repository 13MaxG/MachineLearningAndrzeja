{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339L, 784L)\n",
      "(1850L, 784L)\n"
     ]
    }
   ],
   "source": [
    "# Two-class MNIST \n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "d1 = 5\n",
    "d2 = 6\n",
    "\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
    "\n",
    "X_train = (mnist_x_train.astype('float32') / 255.).reshape((len(mnist_x_train), np.prod(mnist_x_train.shape[1:])))\n",
    "y_train = mnist_y_train\n",
    "X_test = (mnist_x_test.astype('float32') / 255.).reshape((len(mnist_x_test), np.prod(mnist_x_test.shape[1:])))\n",
    "y_test = mnist_y_test\n",
    "\n",
    "X_train = X_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train = y_train[np.logical_or(y_train == d1, y_train == d2)]\n",
    "y_train[y_train==d1] = 0\n",
    "y_train[y_train==d2] = 1\n",
    "X_test = X_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test = y_test[np.logical_or(y_test == d1, y_test == d2)]\n",
    "y_test[y_test==d1] = 0\n",
    "y_test[y_test==d2] = 1\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [5 pkt]\n",
    "\n",
    "Uzupełnij metody forward_pass oraz backward_pass w klasach ReLU, Sigmoid i Dense. Metoda forward_pass ma przyjmować batch inputów i zwracać batch outputów. Metoda backward_pass ma przyjmować batch inputów oraz batch pochodnych cząstkowych outputów i zwracać batch pochodnych cząstkowych inputów oraz wektor (**nie batch**) pochodnych cząstkowych wag. Jeśli wagi przechowujemy w macierzy dwuwymiarowej, to możemy najpierw policzyć pochodne cząstkowe w macierzy o takim samym kształcie, a następnie np. użyć .flat.\n",
    "\n",
    "## Ćwiczenie 2 [4 pkt]\n",
    "\n",
    "Uzupełnij metodę _forward_pass klasy Network. Metoda ta ma przyjmować batch inputów (X) i zwracać dwie rzeczy:\n",
    "* inps - lista batchów inputów dla każdej warstwy w sieci (włącznie z X); te wartości będziemy używali w metodzie _backward_pass\n",
    "* output - batch outputów z sieci (czyli $\\mathbf{\\hat y}$); output **nie** powinien być ostatnim elementem inps.\n",
    "\n",
    "## Ćwiczenie 3 [5 pkt]\n",
    "\n",
    "Uzupełnij metodę _backward_pass klasy Network. Zwróć uwagę, że pochodna funkcji kosztu po neuronach ostatniej warstwy jest już liczona w metodzie _fit_on_batch. Metoda ma zwracać listę layer_grads, której elementy to wektory pochodnych cząstkowych funkcji kosztu po kolejnych warstwach (zwrócone przez metodę Layer.backward_pass). Kolejność wektorów w tej liście ma być zgodna z kolejnością warstw w sieci.\n",
    "\n",
    "## Ćwiczenie 4 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą i aktywacją Sigmoid na powyższych danych (dwuklasowy MNIST). Użyj MSE jako funkcji kosztu (oznacza to regresję do numeru klasy, co jest złym pomysłem, ale póki nie mamy klasy Crossentropy musi nam to wystarczyć). Użyj GD. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 5 [3 pkt]\n",
    "Uzupełnić klasę Crossentropy, wzorując się na klasie MSE.\n",
    "\n",
    "## Ćwiczenie  6 [3 pkt]\n",
    "Uzupełnić klasę Momentum, wzorując się na klasie GD. Wzory można znaleźć tutaj: http://distill.pub/2017/momentum/\n",
    "\n",
    "## Ćwiczenie 7 [3 pkt]\n",
    "Naucz sieć neuronową z jedną warstwą ukrytą. Rozważ dwa przypadki: aktywację ReLU oraz Sigmoid. Czy jest sens używać ReLU jako ostatnią warstwę? Użyj Crossentropy jako funkcji kosztu. Użyj Momentum. Reportuj loss oraz accuracy.\n",
    "\n",
    "## Ćwiczenie 8 [6 pkt]\n",
    "Vanishing gradient.\n",
    "\n",
    "Zadanie polega na zbadaniu zjawiska *vanishing gradient* w głębokich sieciach. Należy zmodyfikować kod warstwy Dense i dodać monitorowanie **normy euklidesowej** wektora delta_weights. Każdą warstwę Dense w trenowanej sieci należy monitorować oddzielnie. Po każdym wywołaniu metody fit_on_batch każdy z monitorów powinien zapamiętać nową normę. Po nauczeniu sieci dla każdej warstwy należy narysować wykres: poziomo - numer wywołania fit_on_batch, pionowo - norma delta_weights. Im niżej znajduje się warstwa Dense, tym silniej będzie zachodziło zjawisko *vanishing gradient*.\n",
    "\n",
    "Naucz dwuwarstwową sieć z aktywacjami Sigmoid, reportując normy delta_weights. Powtórz to dla głębszej sieci (np. 6-10 warstw).\n",
    "\n",
    "## Ćwiczenie 9 [4 pkt]\n",
    "Przetestować kod z ćwiczenia 7. (dwuwarstwowa sieć) stosując inne inicjalizacje wag w warstwach Dense. Napisać własną inicjalizację wag, która sprawi, że sieć niczego się nie nauczy (init='stupid').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warstwy\n",
    "import math\n",
    "import numpy as np\n",
    "class Layer():\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        # return output\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        # return input_grad, weight_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_weights(self, delta_weights):\n",
    "        pass\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        return None\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        return np.array([ [ max(0,x) for x in y] for y in input])\n",
    "        \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i],  [ 1 if x>=0 else 0 for x in input[i]]   ) for i in xrange(len(input))])    \n",
    "        \n",
    "\n",
    "class Sigmoid(Layer):\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        return np.array([ [ 1.0/(1.0+math.exp(-x)) for x in y] for y in input])\n",
    "        pass\n",
    "\n",
    "    def backward_pass(self, input, output_grad):\n",
    "        return  np.array([ np.multiply( output_grad[i],  [ 1.0/(1.0+math.exp(-x)) * (1.0 - 1.0/(1.0+math.exp(-x)) )    for x in input[i]]   ) for i in xrange(len(input))])    \n",
    "        \n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, init = 'gaussian'):\n",
    "        input_size += 1\n",
    "        if init == 'zeros':\n",
    "            self.weights = np.zeros((input_size, output_size))\n",
    "        elif init == 'gaussian':\n",
    "            np.random.seed(1)\n",
    "            self.weights = np.random.normal(\n",
    "                0.,\n",
    "                2. / (input_size + output_size),\n",
    "                (input_size, output_size)\n",
    "            )\n",
    "        elif init == 'aaa':\n",
    "            # Ćwiczenie 8\n",
    "            raise NotImplementedError()            \n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        #self.weights = np.asmatrix(self.weights)\n",
    "\n",
    "    def forward_pass(self, input):\n",
    "        return  np.array([ np.dot(input[i], self.weights)   for i in xrange(len(input))])    \n",
    "        \n",
    "    \n",
    "    def backward_pass(self, input, output_grad):\n",
    "        N = len(input)\n",
    "        n = len(input[0])\n",
    "        m = len(output_grad[0])\n",
    "        input_grad = np.array([ [  np.dot(output_grad[a], self.weights[b])   for b in xrange(n)]   for a in xrange(N)])    \n",
    "        weight_grad=  np.array( [ [  np.dot(output_grad[:,b], input[:,a])  for b in xrange(m)]  for a in xrange(n) ] )  \n",
    "        return input_grad[:,1:], weight_grad#.flat\n",
    "\n",
    "    def backward_pass_2(self, input, output_grad):\n",
    "        input_grad = np.dot(output_grad, self.weights.T)\n",
    "        weight_grad= np.dot(input.T, output_grad)\n",
    "    \n",
    "        return input_grad[:,1:], weight_grad#.flat\n",
    "        \n",
    "    def update_weights(self, delta_weights):\n",
    "        # Ćwiczenie 7 - monitorowanie normy wektora delta_weights\n",
    "        self.weights += delta_weights.reshape(self.weights.shape)\n",
    "\n",
    "    def debug_grad(self, evaluate_loss):\n",
    "        base = evaluate_loss()\n",
    "        grad = []\n",
    "        for (x, y), w in np.ndenumerate(self.weights):\n",
    "            self.weights[x, y] = w + 0.0001\n",
    "            changed = evaluate_loss()\n",
    "            grad.append(10000. * (changed - base))\n",
    "            self.weights[x, y] = w\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.53371346 -0.47871036  0.32786829]\n",
      " [-1.06742692 -0.95742071  0.65573658]] [[5 5]\n",
      " [5 5]\n",
      " [5 5]\n",
      " [0 0]]\n",
      "---\n",
      "[[ 0.          0.          0.        ]\n",
      " [-0.53371346 -0.47871036  0.32786829]\n",
      " [-1.06742692 -0.95742071  0.65573658]] [[5 5]\n",
      " [5 5]\n",
      " [5 5]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(ReLU().forward_pass(np.array([ [-1,-2], [1,0], [2,2] ])), np.array([[0, 0], [1, 0], [2, 2]]))\n",
    "assert np.allclose(Sigmoid().forward_pass(np.array([ [0,0,0], [1,1,1], [2,2,2] ])), np.array([[ 0.5       ,  0.5       ,  0.5       ],[ 0.73105858,  0.73105858,  0.73105858],[ 0.88079708,  0.88079708,  0.88079708]]))\n",
    "assert np.allclose(ReLU().backward_pass(np.array([ [-5,-3], [1.2,2], [-2,2.5] ]), np.array([ [1,-2], [1.1,0], [-2,2.7] ])), np.array([[ 0. , -0. ],[ 1.1,  0. ],[-0. ,  2.7]]) )\n",
    "assert np.allclose(Sigmoid().backward_pass(np.array([ [0,0,0], [1,1,1], [2,2,2] ]), np.array([ [0,0,0], [1,1,1], [2,2,2] ])), np.array([[ 0.        ,  0.        ,  0.        ],[ 0.19661193,  0.19661193,  0.19661193], [ 0.20998717,  0.20998717,  0.20998717]]))\n",
    "#print Dense(input_size=3,output_size=2).forward_pass( np.array( [  [0,0,0,0],[1,1,1,0],[2,2,2,0]  ] ) )\n",
    "tmp1, tmp2= Dense(input_size=3,output_size=2).backward_pass( np.array( [  [0,0,0,0],[1,1,1,0],[2,2,2,0]  ] ), np.array( [  [0,0],[1,1],[2,2]  ] ) )\n",
    "print tmp1, tmp2\n",
    "print \"---\"\n",
    "tmp1, tmp2= Dense(input_size=3,output_size=2).backward_pass_2( np.array( [  [0,0,0,0],[1,1,1,0],[2,2,2,0]  ] ), np.array( [  [0,0],[1,1],[2,2]  ] ) )\n",
    "print tmp1, tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "class Optimizer():\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class GD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_deltas(self, grad):\n",
    "        return -self.learning_rate * grad\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha, beta):\n",
    "        # Ćwiczenie 6\n",
    "        pass\n",
    "        \n",
    "    def calculate_deltas(self, grad):\n",
    "        # Ćwiczenie 6\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funkcje kosztu\n",
    "\n",
    "class Loss():\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # return cost\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        # return y_grad\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MSE(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        return np.average(0.5 * np.square(y - t))\n",
    "\n",
    "    def backward_pass(self, y, t):\n",
    "        return (y - t) / y.size\n",
    "\n",
    "class Crossentropy(Loss):\n",
    "\n",
    "    def forward_pass(self, y, t):\n",
    "        # Ćwiczenie 5\n",
    "        pass\n",
    "        \n",
    "    def backward_pass(self, y, t):\n",
    "        # Ćwiczenie 5\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, loss, optimizer, metrics = []):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def fit(self, X, t, epochs, batch_size=256, print_stats=False):\n",
    "        X = np.array(X)\n",
    "        t = np.array(t)\n",
    "        X = X.reshape(len(X), -1)\n",
    "        t = t.reshape(len(t), -1)\n",
    "        if X.shape[0] != t.shape[0]:\n",
    "            raise ValueError(\"Array sizes don't match\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if print_stats:\n",
    "                print(\"Epoch %d\" % (epoch+1))\n",
    "                print(\"    -> batch size: %d\" % batch_size)\n",
    "            rng_state = np.random.get_state()\n",
    "            np.random.shuffle(X)\n",
    "            np.random.set_state(rng_state)\n",
    "            np.random.shuffle(t)\n",
    "            pos = 0\n",
    "            while pos < len(X):\n",
    "                batch_X = X[pos:pos+batch_size]\n",
    "                batch_t = t[pos:pos+batch_size]\n",
    "                self._fit_on_batch(batch_X, batch_t)\n",
    "                pos += batch_size\n",
    "            if print_stats:\n",
    "                _, y = self._forward_pass(X)\n",
    "                l = self.loss.forward_pass(y, t)\n",
    "                print(\"    -> loss: %f\" % l)\n",
    "                for m in self.metrics:\n",
    "                    print(\"    -> %s: %f\" % (m.__name__, m(y, t)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        inps, out = self._forward_pass(X)\n",
    "        return out\n",
    "\n",
    "    def _fit_on_batch(self, batch_X, batch_t):\n",
    "        inps, out = self._forward_pass(batch_X)\n",
    "        layer_grads = self._backward_pass(\n",
    "            inps,\n",
    "            self.loss.backward_pass(out, batch_t)\n",
    "        )\n",
    "        grad = self._join(layer_grads)\n",
    "        deltas = self.optimizer.calculate_deltas(grad)\n",
    "        for l, d in zip(self.layers, self._split(deltas, layer_grads)):\n",
    "            if not d is None:\n",
    "                l.update_weights(d)\n",
    "\n",
    "    def _join(self, grads):\n",
    "        return np.concatenate([g for g in grads if not g is None])\n",
    "\n",
    "    def _split(self, grads, layer_grads):\n",
    "        out = []\n",
    "        start = 0\n",
    "        for l in layer_grads:\n",
    "            if l is None:\n",
    "                out.append(None)\n",
    "            else:\n",
    "                out.append(grads[start:start+len(l)])\n",
    "                start += len(l)\n",
    "        return out\n",
    "\n",
    "    def _forward_pass(self, X):\n",
    "        inps = []\n",
    "        output = None\n",
    "\n",
    "        # Ćwiczenie 2\n",
    "        inps.append(X)\n",
    "        for layer in self.layers:\n",
    "            inps.append(layer.forward_pass(inps[-1]))\n",
    "            \n",
    "        output = inps[-1]\n",
    "        inps.pop()\n",
    "        return inps, output\n",
    "\n",
    "    def _backward_pass(self, inps, grad):\n",
    "        n = len(self.layers)\n",
    "        layer_grads = [0 for i in xrange(n)]\n",
    "        \n",
    "        layer_grads[n-1] = grad\n",
    "        # Ćwiczenie 3\n",
    "        for i in xrange(n-1, 1,-1):\n",
    "            input_grad, weights_grad = self.layers[i-1].backward_pass(insp[i-1], layer_grads[i]  ) \n",
    "            layer_grads[i-1] =  input_grad\n",
    "        return layer_grads\n",
    "\n",
    "    def _debug_grads(self, X, t):\n",
    "        layer_grads = []\n",
    "        for l in self.layers:\n",
    "            g = l.debug_grad(\n",
    "                lambda: self.loss.forward_pass(self._forward_pass(X)[1], t)\n",
    "            )\n",
    "            if not g is None:\n",
    "                g = np.array(np.array(g).flat)\n",
    "            layer_grads.append(g)\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 4\n",
    "network = Network(loss=MSE(), optimizer=GD(learning_rate=0.01))\n",
    "network.add(Dense(2,1))\n",
    "network.add(Sigmoid())\n",
    "network.fit(X,t, epochs= 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ćwiczenie 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
